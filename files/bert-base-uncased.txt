def @main(%input_ids: Tensor[(1, 512), int64] /* ty=Tensor[(1, 512), int64] span=/bert/embeddings/word_embeddings/Gather.input_ids:0:0 */, %bert.embeddings.word_embeddings.weight: Tensor[(30522, 768), float32] /* ty=Tensor[(30522, 768), float32] span=/bert/embeddings/word_embeddings/Gather.bert.embeddings.word_embeddings.weight:0:0 */, %bert.embeddings.position_embeddings.weight: Tensor[(512, 768), float32] /* ty=Tensor[(512, 768), float32] span=/bert/embeddings/position_embeddings/Gather.bert.embeddings.position_embeddings.weight:0:0 */, %bert.embeddings.token_type_embeddings.weight: Tensor[(2, 768), float32] /* ty=Tensor[(2, 768), float32] span=/bert/embeddings/token_type_embeddings/Gather.bert.embeddings.token_type_embeddings.weight:0:0 */, %bert.embeddings.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/embeddings/LayerNorm/LayerNormalization.bert.embeddings.LayerNorm.weight:0:0 */, %bert.embeddings.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/embeddings/LayerNorm/LayerNormalization.bert.embeddings.LayerNorm.bias:0:0 */, %bert.encoder.layer.0.attention.self.query.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.0/attention/self/query/Add.bert.encoder.layer.0.attention.self.query.bias:0:0 */, %bert.encoder.layer.0.attention.self.key.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.0/attention/self/key/Add.bert.encoder.layer.0.attention.self.key.bias:0:0 */, %bert.encoder.layer.0.attention.self.value.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.0/attention/self/value/Add.bert.encoder.layer.0.attention.self.value.bias:0:0 */, %bert.encoder.layer.0.attention.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.0/attention/output/dense/Add.bert.encoder.layer.0.attention.output.dense.bias:0:0 */, %bert.encoder.layer.0.attention.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.0/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.0.attention.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.0.attention.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.0/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.0.attention.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.0.intermediate.dense.bias: Tensor[(3072), float32] /* ty=Tensor[(3072), float32] span=/bert/encoder/layer.0/intermediate/dense/Add.bert.encoder.layer.0.intermediate.dense.bias:0:0 */, %bert.encoder.layer.0.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.0/output/dense/Add.bert.encoder.layer.0.output.dense.bias:0:0 */, %bert.encoder.layer.0.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.0/output/LayerNorm/LayerNormalization.bert.encoder.layer.0.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.0.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.0/output/LayerNorm/LayerNormalization.bert.encoder.layer.0.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.1.attention.self.query.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.1/attention/self/query/Add.bert.encoder.layer.1.attention.self.query.bias:0:0 */, %bert.encoder.layer.1.attention.self.key.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.1/attention/self/key/Add.bert.encoder.layer.1.attention.self.key.bias:0:0 */, %bert.encoder.layer.1.attention.self.value.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.1/attention/self/value/Add.bert.encoder.layer.1.attention.self.value.bias:0:0 */, %bert.encoder.layer.1.attention.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.1/attention/output/dense/Add.bert.encoder.layer.1.attention.output.dense.bias:0:0 */, %bert.encoder.layer.1.attention.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.1/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.1.attention.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.1.attention.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.1/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.1.attention.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.1.intermediate.dense.bias: Tensor[(3072), float32] /* ty=Tensor[(3072), float32] span=/bert/encoder/layer.1/intermediate/dense/Add.bert.encoder.layer.1.intermediate.dense.bias:0:0 */, %bert.encoder.layer.1.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.1/output/dense/Add.bert.encoder.layer.1.output.dense.bias:0:0 */, %bert.encoder.layer.1.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.1/output/LayerNorm/LayerNormalization.bert.encoder.layer.1.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.1.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.1/output/LayerNorm/LayerNormalization.bert.encoder.layer.1.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.2.attention.self.query.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.2/attention/self/query/Add.bert.encoder.layer.2.attention.self.query.bias:0:0 */, %bert.encoder.layer.2.attention.self.key.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.2/attention/self/key/Add.bert.encoder.layer.2.attention.self.key.bias:0:0 */, %bert.encoder.layer.2.attention.self.value.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.2/attention/self/value/Add.bert.encoder.layer.2.attention.self.value.bias:0:0 */, %bert.encoder.layer.2.attention.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.2/attention/output/dense/Add.bert.encoder.layer.2.attention.output.dense.bias:0:0 */, %bert.encoder.layer.2.attention.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.2/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.2.attention.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.2.attention.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.2/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.2.attention.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.2.intermediate.dense.bias: Tensor[(3072), float32] /* ty=Tensor[(3072), float32] span=/bert/encoder/layer.2/intermediate/dense/Add.bert.encoder.layer.2.intermediate.dense.bias:0:0 */, %bert.encoder.layer.2.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.2/output/dense/Add.bert.encoder.layer.2.output.dense.bias:0:0 */, %bert.encoder.layer.2.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.2/output/LayerNorm/LayerNormalization.bert.encoder.layer.2.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.2.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.2/output/LayerNorm/LayerNormalization.bert.encoder.layer.2.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.3.attention.self.query.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.3/attention/self/query/Add.bert.encoder.layer.3.attention.self.query.bias:0:0 */, %bert.encoder.layer.3.attention.self.key.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.3/attention/self/key/Add.bert.encoder.layer.3.attention.self.key.bias:0:0 */, %bert.encoder.layer.3.attention.self.value.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.3/attention/self/value/Add.bert.encoder.layer.3.attention.self.value.bias:0:0 */, %bert.encoder.layer.3.attention.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.3/attention/output/dense/Add.bert.encoder.layer.3.attention.output.dense.bias:0:0 */, %bert.encoder.layer.3.attention.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.3/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.3.attention.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.3.attention.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.3/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.3.attention.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.3.intermediate.dense.bias: Tensor[(3072), float32] /* ty=Tensor[(3072), float32] span=/bert/encoder/layer.3/intermediate/dense/Add.bert.encoder.layer.3.intermediate.dense.bias:0:0 */, %bert.encoder.layer.3.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.3/output/dense/Add.bert.encoder.layer.3.output.dense.bias:0:0 */, %bert.encoder.layer.3.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.3/output/LayerNorm/LayerNormalization.bert.encoder.layer.3.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.3.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.3/output/LayerNorm/LayerNormalization.bert.encoder.layer.3.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.4.attention.self.query.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.4/attention/self/query/Add.bert.encoder.layer.4.attention.self.query.bias:0:0 */, %bert.encoder.layer.4.attention.self.key.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.4/attention/self/key/Add.bert.encoder.layer.4.attention.self.key.bias:0:0 */, %bert.encoder.layer.4.attention.self.value.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.4/attention/self/value/Add.bert.encoder.layer.4.attention.self.value.bias:0:0 */, %bert.encoder.layer.4.attention.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.4/attention/output/dense/Add.bert.encoder.layer.4.attention.output.dense.bias:0:0 */, %bert.encoder.layer.4.attention.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.4/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.4.attention.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.4.attention.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.4/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.4.attention.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.4.intermediate.dense.bias: Tensor[(3072), float32] /* ty=Tensor[(3072), float32] span=/bert/encoder/layer.4/intermediate/dense/Add.bert.encoder.layer.4.intermediate.dense.bias:0:0 */, %bert.encoder.layer.4.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.4/output/dense/Add.bert.encoder.layer.4.output.dense.bias:0:0 */, %bert.encoder.layer.4.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.4/output/LayerNorm/LayerNormalization.bert.encoder.layer.4.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.4.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.4/output/LayerNorm/LayerNormalization.bert.encoder.layer.4.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.5.attention.self.query.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.5/attention/self/query/Add.bert.encoder.layer.5.attention.self.query.bias:0:0 */, %bert.encoder.layer.5.attention.self.key.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.5/attention/self/key/Add.bert.encoder.layer.5.attention.self.key.bias:0:0 */, %bert.encoder.layer.5.attention.self.value.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.5/attention/self/value/Add.bert.encoder.layer.5.attention.self.value.bias:0:0 */, %bert.encoder.layer.5.attention.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.5/attention/output/dense/Add.bert.encoder.layer.5.attention.output.dense.bias:0:0 */, %bert.encoder.layer.5.attention.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.5/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.5.attention.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.5.attention.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.5/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.5.attention.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.5.intermediate.dense.bias: Tensor[(3072), float32] /* ty=Tensor[(3072), float32] span=/bert/encoder/layer.5/intermediate/dense/Add.bert.encoder.layer.5.intermediate.dense.bias:0:0 */, %bert.encoder.layer.5.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.5/output/dense/Add.bert.encoder.layer.5.output.dense.bias:0:0 */, %bert.encoder.layer.5.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.5/output/LayerNorm/LayerNormalization.bert.encoder.layer.5.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.5.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.5/output/LayerNorm/LayerNormalization.bert.encoder.layer.5.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.6.attention.self.query.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.6/attention/self/query/Add.bert.encoder.layer.6.attention.self.query.bias:0:0 */, %bert.encoder.layer.6.attention.self.key.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.6/attention/self/key/Add.bert.encoder.layer.6.attention.self.key.bias:0:0 */, %bert.encoder.layer.6.attention.self.value.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.6/attention/self/value/Add.bert.encoder.layer.6.attention.self.value.bias:0:0 */, %bert.encoder.layer.6.attention.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.6/attention/output/dense/Add.bert.encoder.layer.6.attention.output.dense.bias:0:0 */, %bert.encoder.layer.6.attention.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.6/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.6.attention.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.6.attention.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.6/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.6.attention.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.6.intermediate.dense.bias: Tensor[(3072), float32] /* ty=Tensor[(3072), float32] span=/bert/encoder/layer.6/intermediate/dense/Add.bert.encoder.layer.6.intermediate.dense.bias:0:0 */, %bert.encoder.layer.6.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.6/output/dense/Add.bert.encoder.layer.6.output.dense.bias:0:0 */, %bert.encoder.layer.6.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.6/output/LayerNorm/LayerNormalization.bert.encoder.layer.6.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.6.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.6/output/LayerNorm/LayerNormalization.bert.encoder.layer.6.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.7.attention.self.query.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.7/attention/self/query/Add.bert.encoder.layer.7.attention.self.query.bias:0:0 */, %bert.encoder.layer.7.attention.self.key.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.7/attention/self/key/Add.bert.encoder.layer.7.attention.self.key.bias:0:0 */, %bert.encoder.layer.7.attention.self.value.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.7/attention/self/value/Add.bert.encoder.layer.7.attention.self.value.bias:0:0 */, %bert.encoder.layer.7.attention.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.7/attention/output/dense/Add.bert.encoder.layer.7.attention.output.dense.bias:0:0 */, %bert.encoder.layer.7.attention.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.7/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.7.attention.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.7.attention.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.7/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.7.attention.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.7.intermediate.dense.bias: Tensor[(3072), float32] /* ty=Tensor[(3072), float32] span=/bert/encoder/layer.7/intermediate/dense/Add.bert.encoder.layer.7.intermediate.dense.bias:0:0 */, %bert.encoder.layer.7.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.7/output/dense/Add.bert.encoder.layer.7.output.dense.bias:0:0 */, %bert.encoder.layer.7.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.7/output/LayerNorm/LayerNormalization.bert.encoder.layer.7.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.7.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.7/output/LayerNorm/LayerNormalization.bert.encoder.layer.7.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.8.attention.self.query.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.8/attention/self/query/Add.bert.encoder.layer.8.attention.self.query.bias:0:0 */, %bert.encoder.layer.8.attention.self.key.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.8/attention/self/key/Add.bert.encoder.layer.8.attention.self.key.bias:0:0 */, %bert.encoder.layer.8.attention.self.value.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.8/attention/self/value/Add.bert.encoder.layer.8.attention.self.value.bias:0:0 */, %bert.encoder.layer.8.attention.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.8/attention/output/dense/Add.bert.encoder.layer.8.attention.output.dense.bias:0:0 */, %bert.encoder.layer.8.attention.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.8/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.8.attention.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.8.attention.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.8/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.8.attention.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.8.intermediate.dense.bias: Tensor[(3072), float32] /* ty=Tensor[(3072), float32] span=/bert/encoder/layer.8/intermediate/dense/Add.bert.encoder.layer.8.intermediate.dense.bias:0:0 */, %bert.encoder.layer.8.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.8/output/dense/Add.bert.encoder.layer.8.output.dense.bias:0:0 */, %bert.encoder.layer.8.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.8/output/LayerNorm/LayerNormalization.bert.encoder.layer.8.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.8.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.8/output/LayerNorm/LayerNormalization.bert.encoder.layer.8.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.9.attention.self.query.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.9/attention/self/query/Add.bert.encoder.layer.9.attention.self.query.bias:0:0 */, %bert.encoder.layer.9.attention.self.key.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.9/attention/self/key/Add.bert.encoder.layer.9.attention.self.key.bias:0:0 */, %bert.encoder.layer.9.attention.self.value.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.9/attention/self/value/Add.bert.encoder.layer.9.attention.self.value.bias:0:0 */, %bert.encoder.layer.9.attention.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.9/attention/output/dense/Add.bert.encoder.layer.9.attention.output.dense.bias:0:0 */, %bert.encoder.layer.9.attention.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.9/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.9.attention.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.9.attention.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.9/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.9.attention.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.9.intermediate.dense.bias: Tensor[(3072), float32] /* ty=Tensor[(3072), float32] span=/bert/encoder/layer.9/intermediate/dense/Add.bert.encoder.layer.9.intermediate.dense.bias:0:0 */, %bert.encoder.layer.9.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.9/output/dense/Add.bert.encoder.layer.9.output.dense.bias:0:0 */, %bert.encoder.layer.9.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.9/output/LayerNorm/LayerNormalization.bert.encoder.layer.9.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.9.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.9/output/LayerNorm/LayerNormalization.bert.encoder.layer.9.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.10.attention.self.query.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.10/attention/self/query/Add.bert.encoder.layer.10.attention.self.query.bias:0:0 */, %bert.encoder.layer.10.attention.self.key.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.10/attention/self/key/Add.bert.encoder.layer.10.attention.self.key.bias:0:0 */, %bert.encoder.layer.10.attention.self.value.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.10/attention/self/value/Add.bert.encoder.layer.10.attention.self.value.bias:0:0 */, %bert.encoder.layer.10.attention.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.10/attention/output/dense/Add.bert.encoder.layer.10.attention.output.dense.bias:0:0 */, %bert.encoder.layer.10.attention.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.10/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.10.attention.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.10.attention.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.10/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.10.attention.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.10.intermediate.dense.bias: Tensor[(3072), float32] /* ty=Tensor[(3072), float32] span=/bert/encoder/layer.10/intermediate/dense/Add.bert.encoder.layer.10.intermediate.dense.bias:0:0 */, %bert.encoder.layer.10.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.10/output/dense/Add.bert.encoder.layer.10.output.dense.bias:0:0 */, %bert.encoder.layer.10.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.10/output/LayerNorm/LayerNormalization.bert.encoder.layer.10.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.10.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.10/output/LayerNorm/LayerNormalization.bert.encoder.layer.10.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.11.attention.self.query.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.11/attention/self/query/Add.bert.encoder.layer.11.attention.self.query.bias:0:0 */, %bert.encoder.layer.11.attention.self.key.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.11/attention/self/key/Add.bert.encoder.layer.11.attention.self.key.bias:0:0 */, %bert.encoder.layer.11.attention.self.value.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.11/attention/self/value/Add.bert.encoder.layer.11.attention.self.value.bias:0:0 */, %bert.encoder.layer.11.attention.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.11/attention/output/dense/Add.bert.encoder.layer.11.attention.output.dense.bias:0:0 */, %bert.encoder.layer.11.attention.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.11/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.11.attention.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.11.attention.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.11/attention/output/LayerNorm/LayerNormalization.bert.encoder.layer.11.attention.output.LayerNorm.bias:0:0 */, %bert.encoder.layer.11.intermediate.dense.bias: Tensor[(3072), float32] /* ty=Tensor[(3072), float32] span=/bert/encoder/layer.11/intermediate/dense/Add.bert.encoder.layer.11.intermediate.dense.bias:0:0 */, %bert.encoder.layer.11.output.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.11/output/dense/Add.bert.encoder.layer.11.output.dense.bias:0:0 */, %bert.encoder.layer.11.output.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.11/output/LayerNorm/LayerNormalization.bert.encoder.layer.11.output.LayerNorm.weight:0:0 */, %bert.encoder.layer.11.output.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/bert/encoder/layer.11/output/LayerNorm/LayerNormalization.bert.encoder.layer.11.output.LayerNorm.bias:0:0 */, %cls.predictions.bias: Tensor[(30522), float32] /* ty=Tensor[(30522), float32] span=/cls/predictions/decoder/Add.cls.predictions.bias:0:0 */, %cls.predictions.transform.dense.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/cls/predictions/transform/dense/Add.cls.predictions.transform.dense.bias:0:0 */, %cls.predictions.transform.LayerNorm.weight: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/cls/predictions/transform/LayerNorm/LayerNormalization.cls.predictions.transform.LayerNorm.weight:0:0 */, %cls.predictions.transform.LayerNorm.bias: Tensor[(768), float32] /* ty=Tensor[(768), float32] span=/cls/predictions/transform/LayerNorm/LayerNormalization.cls.predictions.transform.LayerNorm.bias:0:0 */, %onnx::MatMul_1450: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.0/attention/self/query/MatMul.onnx::MatMul_1450:0:0 */, %onnx::MatMul_1456: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.0/attention/self/key/MatMul.onnx::MatMul_1456:0:0 */, %onnx::MatMul_1462: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.0/attention/self/value/MatMul.onnx::MatMul_1462:0:0 */, %onnx::MatMul_1472: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.0/attention/output/dense/MatMul.onnx::MatMul_1472:0:0 */, %onnx::MatMul_1473: Tensor[(768, 3072), float32] /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.0/intermediate/dense/MatMul.onnx::MatMul_1473:0:0 */, %onnx::MatMul_1474: Tensor[(3072, 768), float32] /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.0/output/dense/MatMul.onnx::MatMul_1474:0:0 */, %onnx::MatMul_1475: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.1/attention/self/query/MatMul.onnx::MatMul_1475:0:0 */, %onnx::MatMul_1481: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.1/attention/self/key/MatMul.onnx::MatMul_1481:0:0 */, %onnx::MatMul_1487: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.1/attention/self/value/MatMul.onnx::MatMul_1487:0:0 */, %onnx::MatMul_1497: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.1/attention/output/dense/MatMul.onnx::MatMul_1497:0:0 */, %onnx::MatMul_1498: Tensor[(768, 3072), float32] /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.1/intermediate/dense/MatMul.onnx::MatMul_1498:0:0 */, %onnx::MatMul_1499: Tensor[(3072, 768), float32] /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.1/output/dense/MatMul.onnx::MatMul_1499:0:0 */, %onnx::MatMul_1500: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.2/attention/self/query/MatMul.onnx::MatMul_1500:0:0 */, %onnx::MatMul_1506: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.2/attention/self/key/MatMul.onnx::MatMul_1506:0:0 */, %onnx::MatMul_1512: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.2/attention/self/value/MatMul.onnx::MatMul_1512:0:0 */, %onnx::MatMul_1522: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.2/attention/output/dense/MatMul.onnx::MatMul_1522:0:0 */, %onnx::MatMul_1523: Tensor[(768, 3072), float32] /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.2/intermediate/dense/MatMul.onnx::MatMul_1523:0:0 */, %onnx::MatMul_1524: Tensor[(3072, 768), float32] /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.2/output/dense/MatMul.onnx::MatMul_1524:0:0 */, %onnx::MatMul_1525: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.3/attention/self/query/MatMul.onnx::MatMul_1525:0:0 */, %onnx::MatMul_1531: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.3/attention/self/key/MatMul.onnx::MatMul_1531:0:0 */, %onnx::MatMul_1537: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.3/attention/self/value/MatMul.onnx::MatMul_1537:0:0 */, %onnx::MatMul_1547: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.3/attention/output/dense/MatMul.onnx::MatMul_1547:0:0 */, %onnx::MatMul_1548: Tensor[(768, 3072), float32] /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.3/intermediate/dense/MatMul.onnx::MatMul_1548:0:0 */, %onnx::MatMul_1549: Tensor[(3072, 768), float32] /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.3/output/dense/MatMul.onnx::MatMul_1549:0:0 */, %onnx::MatMul_1550: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.4/attention/self/query/MatMul.onnx::MatMul_1550:0:0 */, %onnx::MatMul_1556: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.4/attention/self/key/MatMul.onnx::MatMul_1556:0:0 */, %onnx::MatMul_1562: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.4/attention/self/value/MatMul.onnx::MatMul_1562:0:0 */, %onnx::MatMul_1572: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.4/attention/output/dense/MatMul.onnx::MatMul_1572:0:0 */, %onnx::MatMul_1573: Tensor[(768, 3072), float32] /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.4/intermediate/dense/MatMul.onnx::MatMul_1573:0:0 */, %onnx::MatMul_1574: Tensor[(3072, 768), float32] /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.4/output/dense/MatMul.onnx::MatMul_1574:0:0 */, %onnx::MatMul_1575: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.5/attention/self/query/MatMul.onnx::MatMul_1575:0:0 */, %onnx::MatMul_1581: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.5/attention/self/key/MatMul.onnx::MatMul_1581:0:0 */, %onnx::MatMul_1587: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.5/attention/self/value/MatMul.onnx::MatMul_1587:0:0 */, %onnx::MatMul_1597: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.5/attention/output/dense/MatMul.onnx::MatMul_1597:0:0 */, %onnx::MatMul_1598: Tensor[(768, 3072), float32] /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.5/intermediate/dense/MatMul.onnx::MatMul_1598:0:0 */, %onnx::MatMul_1599: Tensor[(3072, 768), float32] /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.5/output/dense/MatMul.onnx::MatMul_1599:0:0 */, %onnx::MatMul_1600: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.6/attention/self/query/MatMul.onnx::MatMul_1600:0:0 */, %onnx::MatMul_1606: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.6/attention/self/key/MatMul.onnx::MatMul_1606:0:0 */, %onnx::MatMul_1612: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.6/attention/self/value/MatMul.onnx::MatMul_1612:0:0 */, %onnx::MatMul_1622: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.6/attention/output/dense/MatMul.onnx::MatMul_1622:0:0 */, %onnx::MatMul_1623: Tensor[(768, 3072), float32] /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.6/intermediate/dense/MatMul.onnx::MatMul_1623:0:0 */, %onnx::MatMul_1624: Tensor[(3072, 768), float32] /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.6/output/dense/MatMul.onnx::MatMul_1624:0:0 */, %onnx::MatMul_1625: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.7/attention/self/query/MatMul.onnx::MatMul_1625:0:0 */, %onnx::MatMul_1631: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.7/attention/self/key/MatMul.onnx::MatMul_1631:0:0 */, %onnx::MatMul_1637: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.7/attention/self/value/MatMul.onnx::MatMul_1637:0:0 */, %onnx::MatMul_1647: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.7/attention/output/dense/MatMul.onnx::MatMul_1647:0:0 */, %onnx::MatMul_1648: Tensor[(768, 3072), float32] /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.7/intermediate/dense/MatMul.onnx::MatMul_1648:0:0 */, %onnx::MatMul_1649: Tensor[(3072, 768), float32] /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.7/output/dense/MatMul.onnx::MatMul_1649:0:0 */, %onnx::MatMul_1650: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.8/attention/self/query/MatMul.onnx::MatMul_1650:0:0 */, %onnx::MatMul_1656: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.8/attention/self/key/MatMul.onnx::MatMul_1656:0:0 */, %onnx::MatMul_1662: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.8/attention/self/value/MatMul.onnx::MatMul_1662:0:0 */, %onnx::MatMul_1672: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.8/attention/output/dense/MatMul.onnx::MatMul_1672:0:0 */, %onnx::MatMul_1673: Tensor[(768, 3072), float32] /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.8/intermediate/dense/MatMul.onnx::MatMul_1673:0:0 */, %onnx::MatMul_1674: Tensor[(3072, 768), float32] /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.8/output/dense/MatMul.onnx::MatMul_1674:0:0 */, %onnx::MatMul_1675: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.9/attention/self/query/MatMul.onnx::MatMul_1675:0:0 */, %onnx::MatMul_1681: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.9/attention/self/key/MatMul.onnx::MatMul_1681:0:0 */, %onnx::MatMul_1687: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.9/attention/self/value/MatMul.onnx::MatMul_1687:0:0 */, %onnx::MatMul_1697: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.9/attention/output/dense/MatMul.onnx::MatMul_1697:0:0 */, %onnx::MatMul_1698: Tensor[(768, 3072), float32] /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.9/intermediate/dense/MatMul.onnx::MatMul_1698:0:0 */, %onnx::MatMul_1699: Tensor[(3072, 768), float32] /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.9/output/dense/MatMul.onnx::MatMul_1699:0:0 */, %onnx::MatMul_1700: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.10/attention/self/query/MatMul.onnx::MatMul_1700:0:0 */, %onnx::MatMul_1706: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.10/attention/self/key/MatMul.onnx::MatMul_1706:0:0 */, %onnx::MatMul_1712: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.10/attention/self/value/MatMul.onnx::MatMul_1712:0:0 */, %onnx::MatMul_1722: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.10/attention/output/dense/MatMul.onnx::MatMul_1722:0:0 */, %onnx::MatMul_1723: Tensor[(768, 3072), float32] /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.10/intermediate/dense/MatMul.onnx::MatMul_1723:0:0 */, %onnx::MatMul_1724: Tensor[(3072, 768), float32] /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.10/output/dense/MatMul.onnx::MatMul_1724:0:0 */, %onnx::MatMul_1725: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.11/attention/self/query/MatMul.onnx::MatMul_1725:0:0 */, %onnx::MatMul_1731: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.11/attention/self/key/MatMul.onnx::MatMul_1731:0:0 */, %onnx::MatMul_1737: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.11/attention/self/value/MatMul.onnx::MatMul_1737:0:0 */, %onnx::MatMul_1747: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.11/attention/output/dense/MatMul.onnx::MatMul_1747:0:0 */, %onnx::MatMul_1748: Tensor[(768, 3072), float32] /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.11/intermediate/dense/MatMul.onnx::MatMul_1748:0:0 */, %onnx::MatMul_1749: Tensor[(3072, 768), float32] /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.11/output/dense/MatMul.onnx::MatMul_1749:0:0 */, %onnx::MatMul_1750: Tensor[(768, 768), float32] /* ty=Tensor[(768, 768), float32] span=/cls/predictions/transform/dense/MatMul.onnx::MatMul_1750:0:0 */, %onnx::MatMul_1751: Tensor[(768, 30522), float32] /* ty=Tensor[(768, 30522), float32] span=/cls/predictions/decoder/MatMul.onnx::MatMul_1751:0:0 */) -> Tensor[(1, 512, 30522), float32] {
  %0 = less(%input_ids, 0i64 /* ty=int64 span=/bert/embeddings/word_embeddings/Gather:0:0 */) /* ty=Tensor[(1, 512), bool] span=/bert/embeddings/word_embeddings/Gather:0:0 */;
  %1 = add(%input_ids, 30522i64 /* ty=int64 */) /* ty=Tensor[(1, 512), int64] span=/bert/embeddings/word_embeddings/Gather:0:0 */;
  %2 = where(%0, %1, %input_ids) /* ty=Tensor[(1, 512), int64] span=/bert/embeddings/word_embeddings/Gather:0:0 */;
  %3 = take(%bert.embeddings.word_embeddings.weight, %2, axis=0) /* ty=Tensor[(1, 512, 768), float32] span=/bert/embeddings/word_embeddings/Gather:0:0 */;
  %4 = take(%bert.embeddings.token_type_embeddings.weight, meta[relay.Constant][0] /* ty=Tensor[(1, 512), int64] */, axis=0) /* ty=Tensor[(1, 512, 768), float32] span=/bert/embeddings/token_type_embeddings/Gather:0:0 */;
  %5 = add(%3, %4) /* ty=Tensor[(1, 512, 768), float32] span=/bert/embeddings/Add:0:0 */;
  %6 = take(%bert.embeddings.position_embeddings.weight, meta[relay.Constant][1] /* ty=Tensor[(1, 512), int64] */, axis=0) /* ty=Tensor[(1, 512, 768), float32] span=/bert/embeddings/position_embeddings/Gather:0:0 */;
  %7 = add(%5, %6) /* ty=Tensor[(1, 512, 768), float32] span=/bert/embeddings/Add_1:0:0 */;
  %8 = mean(%7, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/embeddings/LayerNorm/LayerNormalization:0:0 */;
  %9 = variance(%7, %8, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/embeddings/LayerNorm/LayerNormalization:0:0 */;
  %10 = add(%9, 1e-12f /* ty=float32 span=/bert/embeddings/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/embeddings/LayerNorm/LayerNormalization:0:0 */;
  %11 = sqrt(%10) /* ty=Tensor[(1, 512, 1), float32] span=/bert/embeddings/LayerNorm/LayerNormalization:0:0 */;
  %12 = subtract(%7, %8) /* ty=Tensor[(1, 512, 768), float32] span=/bert/embeddings/LayerNorm/LayerNormalization:0:0 */;
  %13 = divide(1f /* ty=float32 span=/bert/embeddings/LayerNorm/LayerNormalization:0:0 */, %11) /* ty=Tensor[(1, 512, 1), float32] span=/bert/embeddings/LayerNorm/LayerNormalization:0:0 */;
  %14 = multiply(%12, %13) /* ty=Tensor[(1, 512, 768), float32] span=/bert/embeddings/LayerNorm/LayerNormalization:0:0 */;
  %15 = multiply(%14, %bert.embeddings.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/embeddings/LayerNorm/LayerNormalization:0:0 */;
  %16 = add(%15, %bert.embeddings.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/embeddings/LayerNorm/LayerNormalization:0:0 */;
  %17 = reshape(%16, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.0/attention/self/query/MatMul:0:0 */;
  %18 = transpose(%onnx::MatMul_1450, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.0/attention/self/query/MatMul:0:0 */;
  %19 = nn.dense(%17, %18, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.0/attention/self/query/MatMul:0:0 */;
  %20 = reshape(%19, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/attention/self/query/MatMul:0:0 */;
  %21 = add(%bert.encoder.layer.0.attention.self.query.bias, %20) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/attention/self/query/Add:0:0 */;
  %22 = reshape(%21, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.0/attention/self/Reshape:0:0 */;
  %23 = transpose(%22, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.0/attention/self/Transpose:0:0 */;
  %24 = multiply(%23, meta[relay.Constant][2] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.0/attention/self/Sqrt_1:0:0 */) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.0/attention/self/Mul:0:0 */;
  %25 = reshape(%16, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.0/attention/self/key/MatMul:0:0 */;
  %26 = transpose(%onnx::MatMul_1456, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.0/attention/self/key/MatMul:0:0 */;
  %27 = nn.dense(%25, %26, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.0/attention/self/key/MatMul:0:0 */;
  %28 = reshape(%27, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/attention/self/key/MatMul:0:0 */;
  %29 = add(%bert.encoder.layer.0.attention.self.key.bias, %28) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/attention/self/key/Add:0:0 */;
  %30 = reshape(%29, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.0/attention/self/Reshape_1:0:0 */;
  %31 = transpose(%30, axes=[0, 2, 3, 1]) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.0/attention/self/Transpose_2:0:0 */;
  %32 = multiply(%31, meta[relay.Constant][3] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.0/attention/self/Sqrt_2:0:0 */) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.0/attention/self/Mul_1:0:0 */;
  %33 = reshape(%32, newshape=[-1, 64, 512]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.0/attention/self/MatMul:0:0 */;
  %34 = reshape(%24, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.0/attention/self/MatMul:0:0 */;
  %35 = transpose(%33, axes=[0, 2, 1]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.0/attention/self/MatMul:0:0 */;
  %36 = nn.batch_matmul(%34, %35, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.0/attention/self/MatMul:0:0 */;
  %37 = reshape(%36, newshape=[1, 12, 512, 512]) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.0/attention/self/MatMul:0:0 */;
  %38 = add(%37, meta[relay.Constant][4] /* ty=Tensor[(1, 1, 512, 512), float32] span=/bert/Where_2:0:0 */) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.0/attention/self/Add:0:0 */;
  %39 = nn.softmax(%38, axis=3) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.0/attention/self/Softmax:0:0 */;
  %40 = reshape(%16, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.0/attention/self/value/MatMul:0:0 */;
  %41 = transpose(%onnx::MatMul_1462, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.0/attention/self/value/MatMul:0:0 */;
  %42 = nn.dense(%40, %41, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.0/attention/self/value/MatMul:0:0 */;
  %43 = reshape(%42, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/attention/self/value/MatMul:0:0 */;
  %44 = add(%bert.encoder.layer.0.attention.self.value.bias, %43) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/attention/self/value/Add:0:0 */;
  %45 = reshape(%44, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.0/attention/self/Reshape_2:0:0 */;
  %46 = transpose(%45, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.0/attention/self/Transpose_1:0:0 */;
  %47 = reshape(%46, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.0/attention/self/MatMul_1:0:0 */;
  %48 = reshape(%39, newshape=[-1, 512, 512]) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.0/attention/self/MatMul_1:0:0 */;
  %49 = transpose(%47, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.0/attention/self/MatMul_1:0:0 */;
  %50 = nn.batch_matmul(%48, %49, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.0/attention/self/MatMul_1:0:0 */;
  %51 = reshape(%50, newshape=[1, 12, 512, 64]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.0/attention/self/MatMul_1:0:0 */;
  %52 = transpose(%51, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.0/attention/self/Transpose_3:0:0 */;
  %53 = reshape(%52, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/attention/self/Reshape_3:0:0 */;
  %54 = reshape(%53, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.0/attention/output/dense/MatMul:0:0 */;
  %55 = transpose(%onnx::MatMul_1472, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.0/attention/output/dense/MatMul:0:0 */;
  %56 = nn.dense(%54, %55, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.0/attention/output/dense/MatMul:0:0 */;
  %57 = reshape(%56, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/attention/output/dense/MatMul:0:0 */;
  %58 = add(%bert.encoder.layer.0.attention.output.dense.bias, %57) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/attention/output/dense/Add:0:0 */;
  %59 = add(%58, %16) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/attention/output/Add:0:0 */;
  %60 = mean(%59, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.0/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %61 = variance(%59, %60, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.0/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %62 = add(%61, 1e-12f /* ty=float32 span=/bert/encoder/layer.0/attention/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.0/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %63 = sqrt(%62) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.0/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %64 = subtract(%59, %60) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %65 = divide(1f /* ty=float32 span=/bert/encoder/layer.0/attention/output/LayerNorm/LayerNormalization:0:0 */, %63) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.0/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %66 = multiply(%64, %65) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %67 = multiply(%66, %bert.encoder.layer.0.attention.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %68 = add(%67, %bert.encoder.layer.0.attention.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %69 = reshape(%68, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.0/intermediate/dense/MatMul:0:0 */;
  %70 = transpose(%onnx::MatMul_1473, axes=None) /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.0/intermediate/dense/MatMul:0:0 */;
  %71 = nn.dense(%69, %70, units=None, out_dtype="float32") /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.0/intermediate/dense/MatMul:0:0 */;
  %72 = reshape(%71, newshape=[1, 512, 3072]) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.0/intermediate/dense/MatMul:0:0 */;
  %73 = add(%bert.encoder.layer.0.intermediate.dense.bias, %72) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.0/intermediate/dense/Add:0:0 */;
  %74 = divide(%73, 1.41421f /* ty=float32 span=/bert/encoder/layer.0/intermediate/intermediate_act_fn/Constant:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.0/intermediate/intermediate_act_fn/Div:0:0 */;
  %75 = erf(%74) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.0/intermediate/intermediate_act_fn/Erf:0:0 */;
  %76 = add(%75, 1f /* ty=float32 span=/bert/encoder/layer.0/intermediate/intermediate_act_fn/Constant_1:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.0/intermediate/intermediate_act_fn/Add:0:0 */;
  %77 = multiply(%73, %76) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.0/intermediate/intermediate_act_fn/Mul:0:0 */;
  %78 = multiply(%77, 0.5f /* ty=float32 span=/bert/encoder/layer.0/intermediate/intermediate_act_fn/Constant_2:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.0/intermediate/intermediate_act_fn/Mul_1:0:0 */;
  %79 = reshape(%78, newshape=[-1, 3072]) /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.0/output/dense/MatMul:0:0 */;
  %80 = transpose(%onnx::MatMul_1474, axes=None) /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.0/output/dense/MatMul:0:0 */;
  %81 = nn.dense(%79, %80, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.0/output/dense/MatMul:0:0 */;
  %82 = reshape(%81, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/output/dense/MatMul:0:0 */;
  %83 = add(%bert.encoder.layer.0.output.dense.bias, %82) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/output/dense/Add:0:0 */;
  %84 = add(%83, %68) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/output/Add:0:0 */;
  %85 = mean(%84, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.0/output/LayerNorm/LayerNormalization:0:0 */;
  %86 = variance(%84, %85, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.0/output/LayerNorm/LayerNormalization:0:0 */;
  %87 = add(%86, 1e-12f /* ty=float32 span=/bert/encoder/layer.0/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.0/output/LayerNorm/LayerNormalization:0:0 */;
  %88 = sqrt(%87) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.0/output/LayerNorm/LayerNormalization:0:0 */;
  %89 = subtract(%84, %85) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/output/LayerNorm/LayerNormalization:0:0 */;
  %90 = divide(1f /* ty=float32 span=/bert/encoder/layer.0/output/LayerNorm/LayerNormalization:0:0 */, %88) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.0/output/LayerNorm/LayerNormalization:0:0 */;
  %91 = multiply(%89, %90) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/output/LayerNorm/LayerNormalization:0:0 */;
  %92 = multiply(%91, %bert.encoder.layer.0.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/output/LayerNorm/LayerNormalization:0:0 */;
  %93 = add(%92, %bert.encoder.layer.0.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.0/output/LayerNorm/LayerNormalization:0:0 */;
  %94 = reshape(%93, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.1/attention/self/query/MatMul:0:0 */;
  %95 = transpose(%onnx::MatMul_1475, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.1/attention/self/query/MatMul:0:0 */;
  %96 = nn.dense(%94, %95, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.1/attention/self/query/MatMul:0:0 */;
  %97 = reshape(%96, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/attention/self/query/MatMul:0:0 */;
  %98 = add(%bert.encoder.layer.1.attention.self.query.bias, %97) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/attention/self/query/Add:0:0 */;
  %99 = reshape(%98, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.1/attention/self/Reshape:0:0 */;
  %100 = transpose(%99, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.1/attention/self/Transpose:0:0 */;
  %101 = multiply(%100, meta[relay.Constant][5] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.1/attention/self/Sqrt_1:0:0 */) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.1/attention/self/Mul:0:0 */;
  %102 = reshape(%93, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.1/attention/self/key/MatMul:0:0 */;
  %103 = transpose(%onnx::MatMul_1481, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.1/attention/self/key/MatMul:0:0 */;
  %104 = nn.dense(%102, %103, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.1/attention/self/key/MatMul:0:0 */;
  %105 = reshape(%104, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/attention/self/key/MatMul:0:0 */;
  %106 = add(%bert.encoder.layer.1.attention.self.key.bias, %105) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/attention/self/key/Add:0:0 */;
  %107 = reshape(%106, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.1/attention/self/Reshape_1:0:0 */;
  %108 = transpose(%107, axes=[0, 2, 3, 1]) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.1/attention/self/Transpose_2:0:0 */;
  %109 = multiply(%108, meta[relay.Constant][6] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.1/attention/self/Sqrt_2:0:0 */) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.1/attention/self/Mul_1:0:0 */;
  %110 = reshape(%109, newshape=[-1, 64, 512]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.1/attention/self/MatMul:0:0 */;
  %111 = reshape(%101, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.1/attention/self/MatMul:0:0 */;
  %112 = transpose(%110, axes=[0, 2, 1]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.1/attention/self/MatMul:0:0 */;
  %113 = nn.batch_matmul(%111, %112, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.1/attention/self/MatMul:0:0 */;
  %114 = reshape(%113, newshape=[1, 12, 512, 512]) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.1/attention/self/MatMul:0:0 */;
  %115 = add(%114, meta[relay.Constant][4] /* ty=Tensor[(1, 1, 512, 512), float32] span=/bert/Where_2:0:0 */) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.1/attention/self/Add:0:0 */;
  %116 = nn.softmax(%115, axis=3) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.1/attention/self/Softmax:0:0 */;
  %117 = reshape(%93, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.1/attention/self/value/MatMul:0:0 */;
  %118 = transpose(%onnx::MatMul_1487, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.1/attention/self/value/MatMul:0:0 */;
  %119 = nn.dense(%117, %118, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.1/attention/self/value/MatMul:0:0 */;
  %120 = reshape(%119, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/attention/self/value/MatMul:0:0 */;
  %121 = add(%bert.encoder.layer.1.attention.self.value.bias, %120) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/attention/self/value/Add:0:0 */;
  %122 = reshape(%121, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.1/attention/self/Reshape_2:0:0 */;
  %123 = transpose(%122, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.1/attention/self/Transpose_1:0:0 */;
  %124 = reshape(%123, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.1/attention/self/MatMul_1:0:0 */;
  %125 = reshape(%116, newshape=[-1, 512, 512]) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.1/attention/self/MatMul_1:0:0 */;
  %126 = transpose(%124, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.1/attention/self/MatMul_1:0:0 */;
  %127 = nn.batch_matmul(%125, %126, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.1/attention/self/MatMul_1:0:0 */;
  %128 = reshape(%127, newshape=[1, 12, 512, 64]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.1/attention/self/MatMul_1:0:0 */;
  %129 = transpose(%128, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.1/attention/self/Transpose_3:0:0 */;
  %130 = reshape(%129, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/attention/self/Reshape_3:0:0 */;
  %131 = reshape(%130, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.1/attention/output/dense/MatMul:0:0 */;
  %132 = transpose(%onnx::MatMul_1497, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.1/attention/output/dense/MatMul:0:0 */;
  %133 = nn.dense(%131, %132, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.1/attention/output/dense/MatMul:0:0 */;
  %134 = reshape(%133, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/attention/output/dense/MatMul:0:0 */;
  %135 = add(%bert.encoder.layer.1.attention.output.dense.bias, %134) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/attention/output/dense/Add:0:0 */;
  %136 = add(%135, %93) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/attention/output/Add:0:0 */;
  %137 = mean(%136, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.1/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %138 = variance(%136, %137, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.1/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %139 = add(%138, 1e-12f /* ty=float32 span=/bert/encoder/layer.1/attention/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.1/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %140 = sqrt(%139) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.1/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %141 = subtract(%136, %137) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %142 = divide(1f /* ty=float32 span=/bert/encoder/layer.1/attention/output/LayerNorm/LayerNormalization:0:0 */, %140) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.1/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %143 = multiply(%141, %142) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %144 = multiply(%143, %bert.encoder.layer.1.attention.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %145 = add(%144, %bert.encoder.layer.1.attention.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %146 = reshape(%145, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.1/intermediate/dense/MatMul:0:0 */;
  %147 = transpose(%onnx::MatMul_1498, axes=None) /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.1/intermediate/dense/MatMul:0:0 */;
  %148 = nn.dense(%146, %147, units=None, out_dtype="float32") /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.1/intermediate/dense/MatMul:0:0 */;
  %149 = reshape(%148, newshape=[1, 512, 3072]) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.1/intermediate/dense/MatMul:0:0 */;
  %150 = add(%bert.encoder.layer.1.intermediate.dense.bias, %149) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.1/intermediate/dense/Add:0:0 */;
  %151 = divide(%150, 1.41421f /* ty=float32 span=/bert/encoder/layer.1/intermediate/intermediate_act_fn/Constant:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.1/intermediate/intermediate_act_fn/Div:0:0 */;
  %152 = erf(%151) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.1/intermediate/intermediate_act_fn/Erf:0:0 */;
  %153 = add(%152, 1f /* ty=float32 span=/bert/encoder/layer.1/intermediate/intermediate_act_fn/Constant_1:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.1/intermediate/intermediate_act_fn/Add:0:0 */;
  %154 = multiply(%150, %153) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.1/intermediate/intermediate_act_fn/Mul:0:0 */;
  %155 = multiply(%154, 0.5f /* ty=float32 span=/bert/encoder/layer.1/intermediate/intermediate_act_fn/Constant_2:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.1/intermediate/intermediate_act_fn/Mul_1:0:0 */;
  %156 = reshape(%155, newshape=[-1, 3072]) /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.1/output/dense/MatMul:0:0 */;
  %157 = transpose(%onnx::MatMul_1499, axes=None) /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.1/output/dense/MatMul:0:0 */;
  %158 = nn.dense(%156, %157, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.1/output/dense/MatMul:0:0 */;
  %159 = reshape(%158, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/output/dense/MatMul:0:0 */;
  %160 = add(%bert.encoder.layer.1.output.dense.bias, %159) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/output/dense/Add:0:0 */;
  %161 = add(%160, %145) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/output/Add:0:0 */;
  %162 = mean(%161, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.1/output/LayerNorm/LayerNormalization:0:0 */;
  %163 = variance(%161, %162, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.1/output/LayerNorm/LayerNormalization:0:0 */;
  %164 = add(%163, 1e-12f /* ty=float32 span=/bert/encoder/layer.1/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.1/output/LayerNorm/LayerNormalization:0:0 */;
  %165 = sqrt(%164) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.1/output/LayerNorm/LayerNormalization:0:0 */;
  %166 = subtract(%161, %162) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/output/LayerNorm/LayerNormalization:0:0 */;
  %167 = divide(1f /* ty=float32 span=/bert/encoder/layer.1/output/LayerNorm/LayerNormalization:0:0 */, %165) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.1/output/LayerNorm/LayerNormalization:0:0 */;
  %168 = multiply(%166, %167) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/output/LayerNorm/LayerNormalization:0:0 */;
  %169 = multiply(%168, %bert.encoder.layer.1.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/output/LayerNorm/LayerNormalization:0:0 */;
  %170 = add(%169, %bert.encoder.layer.1.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.1/output/LayerNorm/LayerNormalization:0:0 */;
  %171 = reshape(%170, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.2/attention/self/query/MatMul:0:0 */;
  %172 = transpose(%onnx::MatMul_1500, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.2/attention/self/query/MatMul:0:0 */;
  %173 = nn.dense(%171, %172, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.2/attention/self/query/MatMul:0:0 */;
  %174 = reshape(%173, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/attention/self/query/MatMul:0:0 */;
  %175 = add(%bert.encoder.layer.2.attention.self.query.bias, %174) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/attention/self/query/Add:0:0 */;
  %176 = reshape(%175, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.2/attention/self/Reshape:0:0 */;
  %177 = transpose(%176, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.2/attention/self/Transpose:0:0 */;
  %178 = multiply(%177, meta[relay.Constant][7] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.2/attention/self/Sqrt_1:0:0 */) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.2/attention/self/Mul:0:0 */;
  %179 = reshape(%170, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.2/attention/self/key/MatMul:0:0 */;
  %180 = transpose(%onnx::MatMul_1506, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.2/attention/self/key/MatMul:0:0 */;
  %181 = nn.dense(%179, %180, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.2/attention/self/key/MatMul:0:0 */;
  %182 = reshape(%181, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/attention/self/key/MatMul:0:0 */;
  %183 = add(%bert.encoder.layer.2.attention.self.key.bias, %182) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/attention/self/key/Add:0:0 */;
  %184 = reshape(%183, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.2/attention/self/Reshape_1:0:0 */;
  %185 = transpose(%184, axes=[0, 2, 3, 1]) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.2/attention/self/Transpose_2:0:0 */;
  %186 = multiply(%185, meta[relay.Constant][8] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.2/attention/self/Sqrt_2:0:0 */) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.2/attention/self/Mul_1:0:0 */;
  %187 = reshape(%186, newshape=[-1, 64, 512]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.2/attention/self/MatMul:0:0 */;
  %188 = reshape(%178, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.2/attention/self/MatMul:0:0 */;
  %189 = transpose(%187, axes=[0, 2, 1]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.2/attention/self/MatMul:0:0 */;
  %190 = nn.batch_matmul(%188, %189, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.2/attention/self/MatMul:0:0 */;
  %191 = reshape(%190, newshape=[1, 12, 512, 512]) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.2/attention/self/MatMul:0:0 */;
  %192 = add(%191, meta[relay.Constant][4] /* ty=Tensor[(1, 1, 512, 512), float32] span=/bert/Where_2:0:0 */) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.2/attention/self/Add:0:0 */;
  %193 = nn.softmax(%192, axis=3) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.2/attention/self/Softmax:0:0 */;
  %194 = reshape(%170, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.2/attention/self/value/MatMul:0:0 */;
  %195 = transpose(%onnx::MatMul_1512, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.2/attention/self/value/MatMul:0:0 */;
  %196 = nn.dense(%194, %195, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.2/attention/self/value/MatMul:0:0 */;
  %197 = reshape(%196, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/attention/self/value/MatMul:0:0 */;
  %198 = add(%bert.encoder.layer.2.attention.self.value.bias, %197) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/attention/self/value/Add:0:0 */;
  %199 = reshape(%198, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.2/attention/self/Reshape_2:0:0 */;
  %200 = transpose(%199, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.2/attention/self/Transpose_1:0:0 */;
  %201 = reshape(%200, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.2/attention/self/MatMul_1:0:0 */;
  %202 = reshape(%193, newshape=[-1, 512, 512]) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.2/attention/self/MatMul_1:0:0 */;
  %203 = transpose(%201, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.2/attention/self/MatMul_1:0:0 */;
  %204 = nn.batch_matmul(%202, %203, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.2/attention/self/MatMul_1:0:0 */;
  %205 = reshape(%204, newshape=[1, 12, 512, 64]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.2/attention/self/MatMul_1:0:0 */;
  %206 = transpose(%205, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.2/attention/self/Transpose_3:0:0 */;
  %207 = reshape(%206, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/attention/self/Reshape_3:0:0 */;
  %208 = reshape(%207, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.2/attention/output/dense/MatMul:0:0 */;
  %209 = transpose(%onnx::MatMul_1522, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.2/attention/output/dense/MatMul:0:0 */;
  %210 = nn.dense(%208, %209, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.2/attention/output/dense/MatMul:0:0 */;
  %211 = reshape(%210, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/attention/output/dense/MatMul:0:0 */;
  %212 = add(%bert.encoder.layer.2.attention.output.dense.bias, %211) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/attention/output/dense/Add:0:0 */;
  %213 = add(%212, %170) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/attention/output/Add:0:0 */;
  %214 = mean(%213, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.2/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %215 = variance(%213, %214, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.2/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %216 = add(%215, 1e-12f /* ty=float32 span=/bert/encoder/layer.2/attention/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.2/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %217 = sqrt(%216) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.2/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %218 = subtract(%213, %214) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %219 = divide(1f /* ty=float32 span=/bert/encoder/layer.2/attention/output/LayerNorm/LayerNormalization:0:0 */, %217) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.2/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %220 = multiply(%218, %219) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %221 = multiply(%220, %bert.encoder.layer.2.attention.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %222 = add(%221, %bert.encoder.layer.2.attention.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %223 = reshape(%222, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.2/intermediate/dense/MatMul:0:0 */;
  %224 = transpose(%onnx::MatMul_1523, axes=None) /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.2/intermediate/dense/MatMul:0:0 */;
  %225 = nn.dense(%223, %224, units=None, out_dtype="float32") /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.2/intermediate/dense/MatMul:0:0 */;
  %226 = reshape(%225, newshape=[1, 512, 3072]) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.2/intermediate/dense/MatMul:0:0 */;
  %227 = add(%bert.encoder.layer.2.intermediate.dense.bias, %226) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.2/intermediate/dense/Add:0:0 */;
  %228 = divide(%227, 1.41421f /* ty=float32 span=/bert/encoder/layer.2/intermediate/intermediate_act_fn/Constant:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.2/intermediate/intermediate_act_fn/Div:0:0 */;
  %229 = erf(%228) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.2/intermediate/intermediate_act_fn/Erf:0:0 */;
  %230 = add(%229, 1f /* ty=float32 span=/bert/encoder/layer.2/intermediate/intermediate_act_fn/Constant_1:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.2/intermediate/intermediate_act_fn/Add:0:0 */;
  %231 = multiply(%227, %230) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.2/intermediate/intermediate_act_fn/Mul:0:0 */;
  %232 = multiply(%231, 0.5f /* ty=float32 span=/bert/encoder/layer.2/intermediate/intermediate_act_fn/Constant_2:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.2/intermediate/intermediate_act_fn/Mul_1:0:0 */;
  %233 = reshape(%232, newshape=[-1, 3072]) /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.2/output/dense/MatMul:0:0 */;
  %234 = transpose(%onnx::MatMul_1524, axes=None) /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.2/output/dense/MatMul:0:0 */;
  %235 = nn.dense(%233, %234, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.2/output/dense/MatMul:0:0 */;
  %236 = reshape(%235, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/output/dense/MatMul:0:0 */;
  %237 = add(%bert.encoder.layer.2.output.dense.bias, %236) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/output/dense/Add:0:0 */;
  %238 = add(%237, %222) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/output/Add:0:0 */;
  %239 = mean(%238, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.2/output/LayerNorm/LayerNormalization:0:0 */;
  %240 = variance(%238, %239, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.2/output/LayerNorm/LayerNormalization:0:0 */;
  %241 = add(%240, 1e-12f /* ty=float32 span=/bert/encoder/layer.2/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.2/output/LayerNorm/LayerNormalization:0:0 */;
  %242 = sqrt(%241) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.2/output/LayerNorm/LayerNormalization:0:0 */;
  %243 = subtract(%238, %239) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/output/LayerNorm/LayerNormalization:0:0 */;
  %244 = divide(1f /* ty=float32 span=/bert/encoder/layer.2/output/LayerNorm/LayerNormalization:0:0 */, %242) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.2/output/LayerNorm/LayerNormalization:0:0 */;
  %245 = multiply(%243, %244) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/output/LayerNorm/LayerNormalization:0:0 */;
  %246 = multiply(%245, %bert.encoder.layer.2.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/output/LayerNorm/LayerNormalization:0:0 */;
  %247 = add(%246, %bert.encoder.layer.2.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.2/output/LayerNorm/LayerNormalization:0:0 */;
  %248 = reshape(%247, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.3/attention/self/query/MatMul:0:0 */;
  %249 = transpose(%onnx::MatMul_1525, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.3/attention/self/query/MatMul:0:0 */;
  %250 = nn.dense(%248, %249, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.3/attention/self/query/MatMul:0:0 */;
  %251 = reshape(%250, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/attention/self/query/MatMul:0:0 */;
  %252 = add(%bert.encoder.layer.3.attention.self.query.bias, %251) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/attention/self/query/Add:0:0 */;
  %253 = reshape(%252, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.3/attention/self/Reshape:0:0 */;
  %254 = transpose(%253, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.3/attention/self/Transpose:0:0 */;
  %255 = multiply(%254, meta[relay.Constant][9] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.3/attention/self/Sqrt_1:0:0 */) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.3/attention/self/Mul:0:0 */;
  %256 = reshape(%247, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.3/attention/self/key/MatMul:0:0 */;
  %257 = transpose(%onnx::MatMul_1531, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.3/attention/self/key/MatMul:0:0 */;
  %258 = nn.dense(%256, %257, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.3/attention/self/key/MatMul:0:0 */;
  %259 = reshape(%258, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/attention/self/key/MatMul:0:0 */;
  %260 = add(%bert.encoder.layer.3.attention.self.key.bias, %259) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/attention/self/key/Add:0:0 */;
  %261 = reshape(%260, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.3/attention/self/Reshape_1:0:0 */;
  %262 = transpose(%261, axes=[0, 2, 3, 1]) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.3/attention/self/Transpose_2:0:0 */;
  %263 = multiply(%262, meta[relay.Constant][10] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.3/attention/self/Sqrt_2:0:0 */) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.3/attention/self/Mul_1:0:0 */;
  %264 = reshape(%263, newshape=[-1, 64, 512]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.3/attention/self/MatMul:0:0 */;
  %265 = reshape(%255, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.3/attention/self/MatMul:0:0 */;
  %266 = transpose(%264, axes=[0, 2, 1]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.3/attention/self/MatMul:0:0 */;
  %267 = nn.batch_matmul(%265, %266, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.3/attention/self/MatMul:0:0 */;
  %268 = reshape(%267, newshape=[1, 12, 512, 512]) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.3/attention/self/MatMul:0:0 */;
  %269 = add(%268, meta[relay.Constant][4] /* ty=Tensor[(1, 1, 512, 512), float32] span=/bert/Where_2:0:0 */) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.3/attention/self/Add:0:0 */;
  %270 = nn.softmax(%269, axis=3) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.3/attention/self/Softmax:0:0 */;
  %271 = reshape(%247, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.3/attention/self/value/MatMul:0:0 */;
  %272 = transpose(%onnx::MatMul_1537, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.3/attention/self/value/MatMul:0:0 */;
  %273 = nn.dense(%271, %272, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.3/attention/self/value/MatMul:0:0 */;
  %274 = reshape(%273, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/attention/self/value/MatMul:0:0 */;
  %275 = add(%bert.encoder.layer.3.attention.self.value.bias, %274) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/attention/self/value/Add:0:0 */;
  %276 = reshape(%275, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.3/attention/self/Reshape_2:0:0 */;
  %277 = transpose(%276, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.3/attention/self/Transpose_1:0:0 */;
  %278 = reshape(%277, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.3/attention/self/MatMul_1:0:0 */;
  %279 = reshape(%270, newshape=[-1, 512, 512]) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.3/attention/self/MatMul_1:0:0 */;
  %280 = transpose(%278, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.3/attention/self/MatMul_1:0:0 */;
  %281 = nn.batch_matmul(%279, %280, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.3/attention/self/MatMul_1:0:0 */;
  %282 = reshape(%281, newshape=[1, 12, 512, 64]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.3/attention/self/MatMul_1:0:0 */;
  %283 = transpose(%282, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.3/attention/self/Transpose_3:0:0 */;
  %284 = reshape(%283, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/attention/self/Reshape_3:0:0 */;
  %285 = reshape(%284, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.3/attention/output/dense/MatMul:0:0 */;
  %286 = transpose(%onnx::MatMul_1547, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.3/attention/output/dense/MatMul:0:0 */;
  %287 = nn.dense(%285, %286, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.3/attention/output/dense/MatMul:0:0 */;
  %288 = reshape(%287, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/attention/output/dense/MatMul:0:0 */;
  %289 = add(%bert.encoder.layer.3.attention.output.dense.bias, %288) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/attention/output/dense/Add:0:0 */;
  %290 = add(%289, %247) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/attention/output/Add:0:0 */;
  %291 = mean(%290, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.3/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %292 = variance(%290, %291, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.3/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %293 = add(%292, 1e-12f /* ty=float32 span=/bert/encoder/layer.3/attention/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.3/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %294 = sqrt(%293) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.3/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %295 = subtract(%290, %291) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %296 = divide(1f /* ty=float32 span=/bert/encoder/layer.3/attention/output/LayerNorm/LayerNormalization:0:0 */, %294) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.3/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %297 = multiply(%295, %296) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %298 = multiply(%297, %bert.encoder.layer.3.attention.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %299 = add(%298, %bert.encoder.layer.3.attention.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %300 = reshape(%299, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.3/intermediate/dense/MatMul:0:0 */;
  %301 = transpose(%onnx::MatMul_1548, axes=None) /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.3/intermediate/dense/MatMul:0:0 */;
  %302 = nn.dense(%300, %301, units=None, out_dtype="float32") /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.3/intermediate/dense/MatMul:0:0 */;
  %303 = reshape(%302, newshape=[1, 512, 3072]) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.3/intermediate/dense/MatMul:0:0 */;
  %304 = add(%bert.encoder.layer.3.intermediate.dense.bias, %303) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.3/intermediate/dense/Add:0:0 */;
  %305 = divide(%304, 1.41421f /* ty=float32 span=/bert/encoder/layer.3/intermediate/intermediate_act_fn/Constant:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.3/intermediate/intermediate_act_fn/Div:0:0 */;
  %306 = erf(%305) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.3/intermediate/intermediate_act_fn/Erf:0:0 */;
  %307 = add(%306, 1f /* ty=float32 span=/bert/encoder/layer.3/intermediate/intermediate_act_fn/Constant_1:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.3/intermediate/intermediate_act_fn/Add:0:0 */;
  %308 = multiply(%304, %307) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.3/intermediate/intermediate_act_fn/Mul:0:0 */;
  %309 = multiply(%308, 0.5f /* ty=float32 span=/bert/encoder/layer.3/intermediate/intermediate_act_fn/Constant_2:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.3/intermediate/intermediate_act_fn/Mul_1:0:0 */;
  %310 = reshape(%309, newshape=[-1, 3072]) /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.3/output/dense/MatMul:0:0 */;
  %311 = transpose(%onnx::MatMul_1549, axes=None) /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.3/output/dense/MatMul:0:0 */;
  %312 = nn.dense(%310, %311, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.3/output/dense/MatMul:0:0 */;
  %313 = reshape(%312, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/output/dense/MatMul:0:0 */;
  %314 = add(%bert.encoder.layer.3.output.dense.bias, %313) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/output/dense/Add:0:0 */;
  %315 = add(%314, %299) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/output/Add:0:0 */;
  %316 = mean(%315, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.3/output/LayerNorm/LayerNormalization:0:0 */;
  %317 = variance(%315, %316, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.3/output/LayerNorm/LayerNormalization:0:0 */;
  %318 = add(%317, 1e-12f /* ty=float32 span=/bert/encoder/layer.3/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.3/output/LayerNorm/LayerNormalization:0:0 */;
  %319 = sqrt(%318) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.3/output/LayerNorm/LayerNormalization:0:0 */;
  %320 = subtract(%315, %316) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/output/LayerNorm/LayerNormalization:0:0 */;
  %321 = divide(1f /* ty=float32 span=/bert/encoder/layer.3/output/LayerNorm/LayerNormalization:0:0 */, %319) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.3/output/LayerNorm/LayerNormalization:0:0 */;
  %322 = multiply(%320, %321) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/output/LayerNorm/LayerNormalization:0:0 */;
  %323 = multiply(%322, %bert.encoder.layer.3.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/output/LayerNorm/LayerNormalization:0:0 */;
  %324 = add(%323, %bert.encoder.layer.3.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.3/output/LayerNorm/LayerNormalization:0:0 */;
  %325 = reshape(%324, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.4/attention/self/query/MatMul:0:0 */;
  %326 = transpose(%onnx::MatMul_1550, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.4/attention/self/query/MatMul:0:0 */;
  %327 = nn.dense(%325, %326, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.4/attention/self/query/MatMul:0:0 */;
  %328 = reshape(%327, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/attention/self/query/MatMul:0:0 */;
  %329 = add(%bert.encoder.layer.4.attention.self.query.bias, %328) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/attention/self/query/Add:0:0 */;
  %330 = reshape(%329, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.4/attention/self/Reshape:0:0 */;
  %331 = transpose(%330, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.4/attention/self/Transpose:0:0 */;
  %332 = multiply(%331, meta[relay.Constant][11] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.4/attention/self/Sqrt_1:0:0 */) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.4/attention/self/Mul:0:0 */;
  %333 = reshape(%324, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.4/attention/self/key/MatMul:0:0 */;
  %334 = transpose(%onnx::MatMul_1556, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.4/attention/self/key/MatMul:0:0 */;
  %335 = nn.dense(%333, %334, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.4/attention/self/key/MatMul:0:0 */;
  %336 = reshape(%335, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/attention/self/key/MatMul:0:0 */;
  %337 = add(%bert.encoder.layer.4.attention.self.key.bias, %336) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/attention/self/key/Add:0:0 */;
  %338 = reshape(%337, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.4/attention/self/Reshape_1:0:0 */;
  %339 = transpose(%338, axes=[0, 2, 3, 1]) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.4/attention/self/Transpose_2:0:0 */;
  %340 = multiply(%339, meta[relay.Constant][12] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.4/attention/self/Sqrt_2:0:0 */) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.4/attention/self/Mul_1:0:0 */;
  %341 = reshape(%340, newshape=[-1, 64, 512]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.4/attention/self/MatMul:0:0 */;
  %342 = reshape(%332, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.4/attention/self/MatMul:0:0 */;
  %343 = transpose(%341, axes=[0, 2, 1]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.4/attention/self/MatMul:0:0 */;
  %344 = nn.batch_matmul(%342, %343, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.4/attention/self/MatMul:0:0 */;
  %345 = reshape(%344, newshape=[1, 12, 512, 512]) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.4/attention/self/MatMul:0:0 */;
  %346 = add(%345, meta[relay.Constant][4] /* ty=Tensor[(1, 1, 512, 512), float32] span=/bert/Where_2:0:0 */) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.4/attention/self/Add:0:0 */;
  %347 = nn.softmax(%346, axis=3) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.4/attention/self/Softmax:0:0 */;
  %348 = reshape(%324, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.4/attention/self/value/MatMul:0:0 */;
  %349 = transpose(%onnx::MatMul_1562, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.4/attention/self/value/MatMul:0:0 */;
  %350 = nn.dense(%348, %349, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.4/attention/self/value/MatMul:0:0 */;
  %351 = reshape(%350, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/attention/self/value/MatMul:0:0 */;
  %352 = add(%bert.encoder.layer.4.attention.self.value.bias, %351) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/attention/self/value/Add:0:0 */;
  %353 = reshape(%352, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.4/attention/self/Reshape_2:0:0 */;
  %354 = transpose(%353, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.4/attention/self/Transpose_1:0:0 */;
  %355 = reshape(%354, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.4/attention/self/MatMul_1:0:0 */;
  %356 = reshape(%347, newshape=[-1, 512, 512]) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.4/attention/self/MatMul_1:0:0 */;
  %357 = transpose(%355, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.4/attention/self/MatMul_1:0:0 */;
  %358 = nn.batch_matmul(%356, %357, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.4/attention/self/MatMul_1:0:0 */;
  %359 = reshape(%358, newshape=[1, 12, 512, 64]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.4/attention/self/MatMul_1:0:0 */;
  %360 = transpose(%359, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.4/attention/self/Transpose_3:0:0 */;
  %361 = reshape(%360, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/attention/self/Reshape_3:0:0 */;
  %362 = reshape(%361, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.4/attention/output/dense/MatMul:0:0 */;
  %363 = transpose(%onnx::MatMul_1572, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.4/attention/output/dense/MatMul:0:0 */;
  %364 = nn.dense(%362, %363, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.4/attention/output/dense/MatMul:0:0 */;
  %365 = reshape(%364, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/attention/output/dense/MatMul:0:0 */;
  %366 = add(%bert.encoder.layer.4.attention.output.dense.bias, %365) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/attention/output/dense/Add:0:0 */;
  %367 = add(%366, %324) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/attention/output/Add:0:0 */;
  %368 = mean(%367, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.4/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %369 = variance(%367, %368, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.4/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %370 = add(%369, 1e-12f /* ty=float32 span=/bert/encoder/layer.4/attention/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.4/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %371 = sqrt(%370) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.4/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %372 = subtract(%367, %368) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %373 = divide(1f /* ty=float32 span=/bert/encoder/layer.4/attention/output/LayerNorm/LayerNormalization:0:0 */, %371) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.4/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %374 = multiply(%372, %373) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %375 = multiply(%374, %bert.encoder.layer.4.attention.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %376 = add(%375, %bert.encoder.layer.4.attention.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %377 = reshape(%376, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.4/intermediate/dense/MatMul:0:0 */;
  %378 = transpose(%onnx::MatMul_1573, axes=None) /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.4/intermediate/dense/MatMul:0:0 */;
  %379 = nn.dense(%377, %378, units=None, out_dtype="float32") /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.4/intermediate/dense/MatMul:0:0 */;
  %380 = reshape(%379, newshape=[1, 512, 3072]) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.4/intermediate/dense/MatMul:0:0 */;
  %381 = add(%bert.encoder.layer.4.intermediate.dense.bias, %380) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.4/intermediate/dense/Add:0:0 */;
  %382 = divide(%381, 1.41421f /* ty=float32 span=/bert/encoder/layer.4/intermediate/intermediate_act_fn/Constant:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.4/intermediate/intermediate_act_fn/Div:0:0 */;
  %383 = erf(%382) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.4/intermediate/intermediate_act_fn/Erf:0:0 */;
  %384 = add(%383, 1f /* ty=float32 span=/bert/encoder/layer.4/intermediate/intermediate_act_fn/Constant_1:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.4/intermediate/intermediate_act_fn/Add:0:0 */;
  %385 = multiply(%381, %384) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.4/intermediate/intermediate_act_fn/Mul:0:0 */;
  %386 = multiply(%385, 0.5f /* ty=float32 span=/bert/encoder/layer.4/intermediate/intermediate_act_fn/Constant_2:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.4/intermediate/intermediate_act_fn/Mul_1:0:0 */;
  %387 = reshape(%386, newshape=[-1, 3072]) /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.4/output/dense/MatMul:0:0 */;
  %388 = transpose(%onnx::MatMul_1574, axes=None) /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.4/output/dense/MatMul:0:0 */;
  %389 = nn.dense(%387, %388, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.4/output/dense/MatMul:0:0 */;
  %390 = reshape(%389, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/output/dense/MatMul:0:0 */;
  %391 = add(%bert.encoder.layer.4.output.dense.bias, %390) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/output/dense/Add:0:0 */;
  %392 = add(%391, %376) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/output/Add:0:0 */;
  %393 = mean(%392, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.4/output/LayerNorm/LayerNormalization:0:0 */;
  %394 = variance(%392, %393, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.4/output/LayerNorm/LayerNormalization:0:0 */;
  %395 = add(%394, 1e-12f /* ty=float32 span=/bert/encoder/layer.4/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.4/output/LayerNorm/LayerNormalization:0:0 */;
  %396 = sqrt(%395) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.4/output/LayerNorm/LayerNormalization:0:0 */;
  %397 = subtract(%392, %393) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/output/LayerNorm/LayerNormalization:0:0 */;
  %398 = divide(1f /* ty=float32 span=/bert/encoder/layer.4/output/LayerNorm/LayerNormalization:0:0 */, %396) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.4/output/LayerNorm/LayerNormalization:0:0 */;
  %399 = multiply(%397, %398) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/output/LayerNorm/LayerNormalization:0:0 */;
  %400 = multiply(%399, %bert.encoder.layer.4.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/output/LayerNorm/LayerNormalization:0:0 */;
  %401 = add(%400, %bert.encoder.layer.4.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.4/output/LayerNorm/LayerNormalization:0:0 */;
  %402 = reshape(%401, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.5/attention/self/query/MatMul:0:0 */;
  %403 = transpose(%onnx::MatMul_1575, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.5/attention/self/query/MatMul:0:0 */;
  %404 = nn.dense(%402, %403, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.5/attention/self/query/MatMul:0:0 */;
  %405 = reshape(%404, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/attention/self/query/MatMul:0:0 */;
  %406 = add(%bert.encoder.layer.5.attention.self.query.bias, %405) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/attention/self/query/Add:0:0 */;
  %407 = reshape(%406, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.5/attention/self/Reshape:0:0 */;
  %408 = transpose(%407, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.5/attention/self/Transpose:0:0 */;
  %409 = multiply(%408, meta[relay.Constant][13] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.5/attention/self/Sqrt_1:0:0 */) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.5/attention/self/Mul:0:0 */;
  %410 = reshape(%401, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.5/attention/self/key/MatMul:0:0 */;
  %411 = transpose(%onnx::MatMul_1581, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.5/attention/self/key/MatMul:0:0 */;
  %412 = nn.dense(%410, %411, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.5/attention/self/key/MatMul:0:0 */;
  %413 = reshape(%412, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/attention/self/key/MatMul:0:0 */;
  %414 = add(%bert.encoder.layer.5.attention.self.key.bias, %413) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/attention/self/key/Add:0:0 */;
  %415 = reshape(%414, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.5/attention/self/Reshape_1:0:0 */;
  %416 = transpose(%415, axes=[0, 2, 3, 1]) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.5/attention/self/Transpose_2:0:0 */;
  %417 = multiply(%416, meta[relay.Constant][14] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.5/attention/self/Sqrt_2:0:0 */) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.5/attention/self/Mul_1:0:0 */;
  %418 = reshape(%417, newshape=[-1, 64, 512]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.5/attention/self/MatMul:0:0 */;
  %419 = reshape(%409, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.5/attention/self/MatMul:0:0 */;
  %420 = transpose(%418, axes=[0, 2, 1]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.5/attention/self/MatMul:0:0 */;
  %421 = nn.batch_matmul(%419, %420, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.5/attention/self/MatMul:0:0 */;
  %422 = reshape(%421, newshape=[1, 12, 512, 512]) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.5/attention/self/MatMul:0:0 */;
  %423 = add(%422, meta[relay.Constant][4] /* ty=Tensor[(1, 1, 512, 512), float32] span=/bert/Where_2:0:0 */) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.5/attention/self/Add:0:0 */;
  %424 = nn.softmax(%423, axis=3) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.5/attention/self/Softmax:0:0 */;
  %425 = reshape(%401, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.5/attention/self/value/MatMul:0:0 */;
  %426 = transpose(%onnx::MatMul_1587, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.5/attention/self/value/MatMul:0:0 */;
  %427 = nn.dense(%425, %426, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.5/attention/self/value/MatMul:0:0 */;
  %428 = reshape(%427, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/attention/self/value/MatMul:0:0 */;
  %429 = add(%bert.encoder.layer.5.attention.self.value.bias, %428) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/attention/self/value/Add:0:0 */;
  %430 = reshape(%429, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.5/attention/self/Reshape_2:0:0 */;
  %431 = transpose(%430, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.5/attention/self/Transpose_1:0:0 */;
  %432 = reshape(%431, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.5/attention/self/MatMul_1:0:0 */;
  %433 = reshape(%424, newshape=[-1, 512, 512]) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.5/attention/self/MatMul_1:0:0 */;
  %434 = transpose(%432, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.5/attention/self/MatMul_1:0:0 */;
  %435 = nn.batch_matmul(%433, %434, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.5/attention/self/MatMul_1:0:0 */;
  %436 = reshape(%435, newshape=[1, 12, 512, 64]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.5/attention/self/MatMul_1:0:0 */;
  %437 = transpose(%436, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.5/attention/self/Transpose_3:0:0 */;
  %438 = reshape(%437, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/attention/self/Reshape_3:0:0 */;
  %439 = reshape(%438, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.5/attention/output/dense/MatMul:0:0 */;
  %440 = transpose(%onnx::MatMul_1597, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.5/attention/output/dense/MatMul:0:0 */;
  %441 = nn.dense(%439, %440, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.5/attention/output/dense/MatMul:0:0 */;
  %442 = reshape(%441, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/attention/output/dense/MatMul:0:0 */;
  %443 = add(%bert.encoder.layer.5.attention.output.dense.bias, %442) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/attention/output/dense/Add:0:0 */;
  %444 = add(%443, %401) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/attention/output/Add:0:0 */;
  %445 = mean(%444, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.5/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %446 = variance(%444, %445, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.5/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %447 = add(%446, 1e-12f /* ty=float32 span=/bert/encoder/layer.5/attention/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.5/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %448 = sqrt(%447) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.5/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %449 = subtract(%444, %445) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %450 = divide(1f /* ty=float32 span=/bert/encoder/layer.5/attention/output/LayerNorm/LayerNormalization:0:0 */, %448) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.5/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %451 = multiply(%449, %450) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %452 = multiply(%451, %bert.encoder.layer.5.attention.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %453 = add(%452, %bert.encoder.layer.5.attention.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %454 = reshape(%453, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.5/intermediate/dense/MatMul:0:0 */;
  %455 = transpose(%onnx::MatMul_1598, axes=None) /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.5/intermediate/dense/MatMul:0:0 */;
  %456 = nn.dense(%454, %455, units=None, out_dtype="float32") /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.5/intermediate/dense/MatMul:0:0 */;
  %457 = reshape(%456, newshape=[1, 512, 3072]) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.5/intermediate/dense/MatMul:0:0 */;
  %458 = add(%bert.encoder.layer.5.intermediate.dense.bias, %457) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.5/intermediate/dense/Add:0:0 */;
  %459 = divide(%458, 1.41421f /* ty=float32 span=/bert/encoder/layer.5/intermediate/intermediate_act_fn/Constant:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.5/intermediate/intermediate_act_fn/Div:0:0 */;
  %460 = erf(%459) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.5/intermediate/intermediate_act_fn/Erf:0:0 */;
  %461 = add(%460, 1f /* ty=float32 span=/bert/encoder/layer.5/intermediate/intermediate_act_fn/Constant_1:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.5/intermediate/intermediate_act_fn/Add:0:0 */;
  %462 = multiply(%458, %461) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.5/intermediate/intermediate_act_fn/Mul:0:0 */;
  %463 = multiply(%462, 0.5f /* ty=float32 span=/bert/encoder/layer.5/intermediate/intermediate_act_fn/Constant_2:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.5/intermediate/intermediate_act_fn/Mul_1:0:0 */;
  %464 = reshape(%463, newshape=[-1, 3072]) /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.5/output/dense/MatMul:0:0 */;
  %465 = transpose(%onnx::MatMul_1599, axes=None) /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.5/output/dense/MatMul:0:0 */;
  %466 = nn.dense(%464, %465, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.5/output/dense/MatMul:0:0 */;
  %467 = reshape(%466, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/output/dense/MatMul:0:0 */;
  %468 = add(%bert.encoder.layer.5.output.dense.bias, %467) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/output/dense/Add:0:0 */;
  %469 = add(%468, %453) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/output/Add:0:0 */;
  %470 = mean(%469, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.5/output/LayerNorm/LayerNormalization:0:0 */;
  %471 = variance(%469, %470, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.5/output/LayerNorm/LayerNormalization:0:0 */;
  %472 = add(%471, 1e-12f /* ty=float32 span=/bert/encoder/layer.5/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.5/output/LayerNorm/LayerNormalization:0:0 */;
  %473 = sqrt(%472) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.5/output/LayerNorm/LayerNormalization:0:0 */;
  %474 = subtract(%469, %470) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/output/LayerNorm/LayerNormalization:0:0 */;
  %475 = divide(1f /* ty=float32 span=/bert/encoder/layer.5/output/LayerNorm/LayerNormalization:0:0 */, %473) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.5/output/LayerNorm/LayerNormalization:0:0 */;
  %476 = multiply(%474, %475) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/output/LayerNorm/LayerNormalization:0:0 */;
  %477 = multiply(%476, %bert.encoder.layer.5.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/output/LayerNorm/LayerNormalization:0:0 */;
  %478 = add(%477, %bert.encoder.layer.5.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.5/output/LayerNorm/LayerNormalization:0:0 */;
  %479 = reshape(%478, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.6/attention/self/query/MatMul:0:0 */;
  %480 = transpose(%onnx::MatMul_1600, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.6/attention/self/query/MatMul:0:0 */;
  %481 = nn.dense(%479, %480, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.6/attention/self/query/MatMul:0:0 */;
  %482 = reshape(%481, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/attention/self/query/MatMul:0:0 */;
  %483 = add(%bert.encoder.layer.6.attention.self.query.bias, %482) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/attention/self/query/Add:0:0 */;
  %484 = reshape(%483, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.6/attention/self/Reshape:0:0 */;
  %485 = transpose(%484, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.6/attention/self/Transpose:0:0 */;
  %486 = multiply(%485, meta[relay.Constant][15] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.6/attention/self/Sqrt_1:0:0 */) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.6/attention/self/Mul:0:0 */;
  %487 = reshape(%478, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.6/attention/self/key/MatMul:0:0 */;
  %488 = transpose(%onnx::MatMul_1606, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.6/attention/self/key/MatMul:0:0 */;
  %489 = nn.dense(%487, %488, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.6/attention/self/key/MatMul:0:0 */;
  %490 = reshape(%489, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/attention/self/key/MatMul:0:0 */;
  %491 = add(%bert.encoder.layer.6.attention.self.key.bias, %490) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/attention/self/key/Add:0:0 */;
  %492 = reshape(%491, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.6/attention/self/Reshape_1:0:0 */;
  %493 = transpose(%492, axes=[0, 2, 3, 1]) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.6/attention/self/Transpose_2:0:0 */;
  %494 = multiply(%493, meta[relay.Constant][16] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.6/attention/self/Sqrt_2:0:0 */) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.6/attention/self/Mul_1:0:0 */;
  %495 = reshape(%494, newshape=[-1, 64, 512]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.6/attention/self/MatMul:0:0 */;
  %496 = reshape(%486, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.6/attention/self/MatMul:0:0 */;
  %497 = transpose(%495, axes=[0, 2, 1]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.6/attention/self/MatMul:0:0 */;
  %498 = nn.batch_matmul(%496, %497, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.6/attention/self/MatMul:0:0 */;
  %499 = reshape(%498, newshape=[1, 12, 512, 512]) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.6/attention/self/MatMul:0:0 */;
  %500 = add(%499, meta[relay.Constant][4] /* ty=Tensor[(1, 1, 512, 512), float32] span=/bert/Where_2:0:0 */) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.6/attention/self/Add:0:0 */;
  %501 = nn.softmax(%500, axis=3) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.6/attention/self/Softmax:0:0 */;
  %502 = reshape(%478, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.6/attention/self/value/MatMul:0:0 */;
  %503 = transpose(%onnx::MatMul_1612, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.6/attention/self/value/MatMul:0:0 */;
  %504 = nn.dense(%502, %503, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.6/attention/self/value/MatMul:0:0 */;
  %505 = reshape(%504, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/attention/self/value/MatMul:0:0 */;
  %506 = add(%bert.encoder.layer.6.attention.self.value.bias, %505) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/attention/self/value/Add:0:0 */;
  %507 = reshape(%506, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.6/attention/self/Reshape_2:0:0 */;
  %508 = transpose(%507, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.6/attention/self/Transpose_1:0:0 */;
  %509 = reshape(%508, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.6/attention/self/MatMul_1:0:0 */;
  %510 = reshape(%501, newshape=[-1, 512, 512]) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.6/attention/self/MatMul_1:0:0 */;
  %511 = transpose(%509, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.6/attention/self/MatMul_1:0:0 */;
  %512 = nn.batch_matmul(%510, %511, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.6/attention/self/MatMul_1:0:0 */;
  %513 = reshape(%512, newshape=[1, 12, 512, 64]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.6/attention/self/MatMul_1:0:0 */;
  %514 = transpose(%513, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.6/attention/self/Transpose_3:0:0 */;
  %515 = reshape(%514, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/attention/self/Reshape_3:0:0 */;
  %516 = reshape(%515, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.6/attention/output/dense/MatMul:0:0 */;
  %517 = transpose(%onnx::MatMul_1622, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.6/attention/output/dense/MatMul:0:0 */;
  %518 = nn.dense(%516, %517, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.6/attention/output/dense/MatMul:0:0 */;
  %519 = reshape(%518, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/attention/output/dense/MatMul:0:0 */;
  %520 = add(%bert.encoder.layer.6.attention.output.dense.bias, %519) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/attention/output/dense/Add:0:0 */;
  %521 = add(%520, %478) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/attention/output/Add:0:0 */;
  %522 = mean(%521, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.6/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %523 = variance(%521, %522, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.6/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %524 = add(%523, 1e-12f /* ty=float32 span=/bert/encoder/layer.6/attention/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.6/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %525 = sqrt(%524) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.6/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %526 = subtract(%521, %522) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %527 = divide(1f /* ty=float32 span=/bert/encoder/layer.6/attention/output/LayerNorm/LayerNormalization:0:0 */, %525) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.6/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %528 = multiply(%526, %527) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %529 = multiply(%528, %bert.encoder.layer.6.attention.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %530 = add(%529, %bert.encoder.layer.6.attention.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %531 = reshape(%530, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.6/intermediate/dense/MatMul:0:0 */;
  %532 = transpose(%onnx::MatMul_1623, axes=None) /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.6/intermediate/dense/MatMul:0:0 */;
  %533 = nn.dense(%531, %532, units=None, out_dtype="float32") /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.6/intermediate/dense/MatMul:0:0 */;
  %534 = reshape(%533, newshape=[1, 512, 3072]) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.6/intermediate/dense/MatMul:0:0 */;
  %535 = add(%bert.encoder.layer.6.intermediate.dense.bias, %534) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.6/intermediate/dense/Add:0:0 */;
  %536 = divide(%535, 1.41421f /* ty=float32 span=/bert/encoder/layer.6/intermediate/intermediate_act_fn/Constant:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.6/intermediate/intermediate_act_fn/Div:0:0 */;
  %537 = erf(%536) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.6/intermediate/intermediate_act_fn/Erf:0:0 */;
  %538 = add(%537, 1f /* ty=float32 span=/bert/encoder/layer.6/intermediate/intermediate_act_fn/Constant_1:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.6/intermediate/intermediate_act_fn/Add:0:0 */;
  %539 = multiply(%535, %538) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.6/intermediate/intermediate_act_fn/Mul:0:0 */;
  %540 = multiply(%539, 0.5f /* ty=float32 span=/bert/encoder/layer.6/intermediate/intermediate_act_fn/Constant_2:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.6/intermediate/intermediate_act_fn/Mul_1:0:0 */;
  %541 = reshape(%540, newshape=[-1, 3072]) /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.6/output/dense/MatMul:0:0 */;
  %542 = transpose(%onnx::MatMul_1624, axes=None) /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.6/output/dense/MatMul:0:0 */;
  %543 = nn.dense(%541, %542, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.6/output/dense/MatMul:0:0 */;
  %544 = reshape(%543, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/output/dense/MatMul:0:0 */;
  %545 = add(%bert.encoder.layer.6.output.dense.bias, %544) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/output/dense/Add:0:0 */;
  %546 = add(%545, %530) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/output/Add:0:0 */;
  %547 = mean(%546, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.6/output/LayerNorm/LayerNormalization:0:0 */;
  %548 = variance(%546, %547, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.6/output/LayerNorm/LayerNormalization:0:0 */;
  %549 = add(%548, 1e-12f /* ty=float32 span=/bert/encoder/layer.6/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.6/output/LayerNorm/LayerNormalization:0:0 */;
  %550 = sqrt(%549) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.6/output/LayerNorm/LayerNormalization:0:0 */;
  %551 = subtract(%546, %547) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/output/LayerNorm/LayerNormalization:0:0 */;
  %552 = divide(1f /* ty=float32 span=/bert/encoder/layer.6/output/LayerNorm/LayerNormalization:0:0 */, %550) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.6/output/LayerNorm/LayerNormalization:0:0 */;
  %553 = multiply(%551, %552) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/output/LayerNorm/LayerNormalization:0:0 */;
  %554 = multiply(%553, %bert.encoder.layer.6.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/output/LayerNorm/LayerNormalization:0:0 */;
  %555 = add(%554, %bert.encoder.layer.6.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.6/output/LayerNorm/LayerNormalization:0:0 */;
  %556 = reshape(%555, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.7/attention/self/query/MatMul:0:0 */;
  %557 = transpose(%onnx::MatMul_1625, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.7/attention/self/query/MatMul:0:0 */;
  %558 = nn.dense(%556, %557, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.7/attention/self/query/MatMul:0:0 */;
  %559 = reshape(%558, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/attention/self/query/MatMul:0:0 */;
  %560 = add(%bert.encoder.layer.7.attention.self.query.bias, %559) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/attention/self/query/Add:0:0 */;
  %561 = reshape(%560, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.7/attention/self/Reshape:0:0 */;
  %562 = transpose(%561, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.7/attention/self/Transpose:0:0 */;
  %563 = multiply(%562, meta[relay.Constant][17] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.7/attention/self/Sqrt_1:0:0 */) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.7/attention/self/Mul:0:0 */;
  %564 = reshape(%555, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.7/attention/self/key/MatMul:0:0 */;
  %565 = transpose(%onnx::MatMul_1631, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.7/attention/self/key/MatMul:0:0 */;
  %566 = nn.dense(%564, %565, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.7/attention/self/key/MatMul:0:0 */;
  %567 = reshape(%566, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/attention/self/key/MatMul:0:0 */;
  %568 = add(%bert.encoder.layer.7.attention.self.key.bias, %567) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/attention/self/key/Add:0:0 */;
  %569 = reshape(%568, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.7/attention/self/Reshape_1:0:0 */;
  %570 = transpose(%569, axes=[0, 2, 3, 1]) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.7/attention/self/Transpose_2:0:0 */;
  %571 = multiply(%570, meta[relay.Constant][18] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.7/attention/self/Sqrt_2:0:0 */) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.7/attention/self/Mul_1:0:0 */;
  %572 = reshape(%571, newshape=[-1, 64, 512]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.7/attention/self/MatMul:0:0 */;
  %573 = reshape(%563, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.7/attention/self/MatMul:0:0 */;
  %574 = transpose(%572, axes=[0, 2, 1]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.7/attention/self/MatMul:0:0 */;
  %575 = nn.batch_matmul(%573, %574, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.7/attention/self/MatMul:0:0 */;
  %576 = reshape(%575, newshape=[1, 12, 512, 512]) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.7/attention/self/MatMul:0:0 */;
  %577 = add(%576, meta[relay.Constant][4] /* ty=Tensor[(1, 1, 512, 512), float32] span=/bert/Where_2:0:0 */) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.7/attention/self/Add:0:0 */;
  %578 = nn.softmax(%577, axis=3) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.7/attention/self/Softmax:0:0 */;
  %579 = reshape(%555, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.7/attention/self/value/MatMul:0:0 */;
  %580 = transpose(%onnx::MatMul_1637, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.7/attention/self/value/MatMul:0:0 */;
  %581 = nn.dense(%579, %580, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.7/attention/self/value/MatMul:0:0 */;
  %582 = reshape(%581, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/attention/self/value/MatMul:0:0 */;
  %583 = add(%bert.encoder.layer.7.attention.self.value.bias, %582) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/attention/self/value/Add:0:0 */;
  %584 = reshape(%583, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.7/attention/self/Reshape_2:0:0 */;
  %585 = transpose(%584, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.7/attention/self/Transpose_1:0:0 */;
  %586 = reshape(%585, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.7/attention/self/MatMul_1:0:0 */;
  %587 = reshape(%578, newshape=[-1, 512, 512]) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.7/attention/self/MatMul_1:0:0 */;
  %588 = transpose(%586, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.7/attention/self/MatMul_1:0:0 */;
  %589 = nn.batch_matmul(%587, %588, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.7/attention/self/MatMul_1:0:0 */;
  %590 = reshape(%589, newshape=[1, 12, 512, 64]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.7/attention/self/MatMul_1:0:0 */;
  %591 = transpose(%590, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.7/attention/self/Transpose_3:0:0 */;
  %592 = reshape(%591, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/attention/self/Reshape_3:0:0 */;
  %593 = reshape(%592, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.7/attention/output/dense/MatMul:0:0 */;
  %594 = transpose(%onnx::MatMul_1647, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.7/attention/output/dense/MatMul:0:0 */;
  %595 = nn.dense(%593, %594, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.7/attention/output/dense/MatMul:0:0 */;
  %596 = reshape(%595, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/attention/output/dense/MatMul:0:0 */;
  %597 = add(%bert.encoder.layer.7.attention.output.dense.bias, %596) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/attention/output/dense/Add:0:0 */;
  %598 = add(%597, %555) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/attention/output/Add:0:0 */;
  %599 = mean(%598, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.7/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %600 = variance(%598, %599, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.7/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %601 = add(%600, 1e-12f /* ty=float32 span=/bert/encoder/layer.7/attention/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.7/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %602 = sqrt(%601) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.7/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %603 = subtract(%598, %599) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %604 = divide(1f /* ty=float32 span=/bert/encoder/layer.7/attention/output/LayerNorm/LayerNormalization:0:0 */, %602) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.7/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %605 = multiply(%603, %604) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %606 = multiply(%605, %bert.encoder.layer.7.attention.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %607 = add(%606, %bert.encoder.layer.7.attention.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %608 = reshape(%607, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.7/intermediate/dense/MatMul:0:0 */;
  %609 = transpose(%onnx::MatMul_1648, axes=None) /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.7/intermediate/dense/MatMul:0:0 */;
  %610 = nn.dense(%608, %609, units=None, out_dtype="float32") /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.7/intermediate/dense/MatMul:0:0 */;
  %611 = reshape(%610, newshape=[1, 512, 3072]) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.7/intermediate/dense/MatMul:0:0 */;
  %612 = add(%bert.encoder.layer.7.intermediate.dense.bias, %611) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.7/intermediate/dense/Add:0:0 */;
  %613 = divide(%612, 1.41421f /* ty=float32 span=/bert/encoder/layer.7/intermediate/intermediate_act_fn/Constant:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.7/intermediate/intermediate_act_fn/Div:0:0 */;
  %614 = erf(%613) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.7/intermediate/intermediate_act_fn/Erf:0:0 */;
  %615 = add(%614, 1f /* ty=float32 span=/bert/encoder/layer.7/intermediate/intermediate_act_fn/Constant_1:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.7/intermediate/intermediate_act_fn/Add:0:0 */;
  %616 = multiply(%612, %615) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.7/intermediate/intermediate_act_fn/Mul:0:0 */;
  %617 = multiply(%616, 0.5f /* ty=float32 span=/bert/encoder/layer.7/intermediate/intermediate_act_fn/Constant_2:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.7/intermediate/intermediate_act_fn/Mul_1:0:0 */;
  %618 = reshape(%617, newshape=[-1, 3072]) /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.7/output/dense/MatMul:0:0 */;
  %619 = transpose(%onnx::MatMul_1649, axes=None) /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.7/output/dense/MatMul:0:0 */;
  %620 = nn.dense(%618, %619, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.7/output/dense/MatMul:0:0 */;
  %621 = reshape(%620, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/output/dense/MatMul:0:0 */;
  %622 = add(%bert.encoder.layer.7.output.dense.bias, %621) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/output/dense/Add:0:0 */;
  %623 = add(%622, %607) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/output/Add:0:0 */;
  %624 = mean(%623, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.7/output/LayerNorm/LayerNormalization:0:0 */;
  %625 = variance(%623, %624, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.7/output/LayerNorm/LayerNormalization:0:0 */;
  %626 = add(%625, 1e-12f /* ty=float32 span=/bert/encoder/layer.7/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.7/output/LayerNorm/LayerNormalization:0:0 */;
  %627 = sqrt(%626) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.7/output/LayerNorm/LayerNormalization:0:0 */;
  %628 = subtract(%623, %624) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/output/LayerNorm/LayerNormalization:0:0 */;
  %629 = divide(1f /* ty=float32 span=/bert/encoder/layer.7/output/LayerNorm/LayerNormalization:0:0 */, %627) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.7/output/LayerNorm/LayerNormalization:0:0 */;
  %630 = multiply(%628, %629) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/output/LayerNorm/LayerNormalization:0:0 */;
  %631 = multiply(%630, %bert.encoder.layer.7.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/output/LayerNorm/LayerNormalization:0:0 */;
  %632 = add(%631, %bert.encoder.layer.7.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.7/output/LayerNorm/LayerNormalization:0:0 */;
  %633 = reshape(%632, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.8/attention/self/query/MatMul:0:0 */;
  %634 = transpose(%onnx::MatMul_1650, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.8/attention/self/query/MatMul:0:0 */;
  %635 = nn.dense(%633, %634, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.8/attention/self/query/MatMul:0:0 */;
  %636 = reshape(%635, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/attention/self/query/MatMul:0:0 */;
  %637 = add(%bert.encoder.layer.8.attention.self.query.bias, %636) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/attention/self/query/Add:0:0 */;
  %638 = reshape(%637, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.8/attention/self/Reshape:0:0 */;
  %639 = transpose(%638, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.8/attention/self/Transpose:0:0 */;
  %640 = multiply(%639, meta[relay.Constant][19] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.8/attention/self/Sqrt_1:0:0 */) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.8/attention/self/Mul:0:0 */;
  %641 = reshape(%632, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.8/attention/self/key/MatMul:0:0 */;
  %642 = transpose(%onnx::MatMul_1656, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.8/attention/self/key/MatMul:0:0 */;
  %643 = nn.dense(%641, %642, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.8/attention/self/key/MatMul:0:0 */;
  %644 = reshape(%643, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/attention/self/key/MatMul:0:0 */;
  %645 = add(%bert.encoder.layer.8.attention.self.key.bias, %644) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/attention/self/key/Add:0:0 */;
  %646 = reshape(%645, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.8/attention/self/Reshape_1:0:0 */;
  %647 = transpose(%646, axes=[0, 2, 3, 1]) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.8/attention/self/Transpose_2:0:0 */;
  %648 = multiply(%647, meta[relay.Constant][20] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.8/attention/self/Sqrt_2:0:0 */) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.8/attention/self/Mul_1:0:0 */;
  %649 = reshape(%648, newshape=[-1, 64, 512]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.8/attention/self/MatMul:0:0 */;
  %650 = reshape(%640, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.8/attention/self/MatMul:0:0 */;
  %651 = transpose(%649, axes=[0, 2, 1]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.8/attention/self/MatMul:0:0 */;
  %652 = nn.batch_matmul(%650, %651, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.8/attention/self/MatMul:0:0 */;
  %653 = reshape(%652, newshape=[1, 12, 512, 512]) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.8/attention/self/MatMul:0:0 */;
  %654 = add(%653, meta[relay.Constant][4] /* ty=Tensor[(1, 1, 512, 512), float32] span=/bert/Where_2:0:0 */) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.8/attention/self/Add:0:0 */;
  %655 = nn.softmax(%654, axis=3) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.8/attention/self/Softmax:0:0 */;
  %656 = reshape(%632, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.8/attention/self/value/MatMul:0:0 */;
  %657 = transpose(%onnx::MatMul_1662, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.8/attention/self/value/MatMul:0:0 */;
  %658 = nn.dense(%656, %657, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.8/attention/self/value/MatMul:0:0 */;
  %659 = reshape(%658, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/attention/self/value/MatMul:0:0 */;
  %660 = add(%bert.encoder.layer.8.attention.self.value.bias, %659) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/attention/self/value/Add:0:0 */;
  %661 = reshape(%660, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.8/attention/self/Reshape_2:0:0 */;
  %662 = transpose(%661, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.8/attention/self/Transpose_1:0:0 */;
  %663 = reshape(%662, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.8/attention/self/MatMul_1:0:0 */;
  %664 = reshape(%655, newshape=[-1, 512, 512]) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.8/attention/self/MatMul_1:0:0 */;
  %665 = transpose(%663, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.8/attention/self/MatMul_1:0:0 */;
  %666 = nn.batch_matmul(%664, %665, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.8/attention/self/MatMul_1:0:0 */;
  %667 = reshape(%666, newshape=[1, 12, 512, 64]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.8/attention/self/MatMul_1:0:0 */;
  %668 = transpose(%667, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.8/attention/self/Transpose_3:0:0 */;
  %669 = reshape(%668, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/attention/self/Reshape_3:0:0 */;
  %670 = reshape(%669, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.8/attention/output/dense/MatMul:0:0 */;
  %671 = transpose(%onnx::MatMul_1672, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.8/attention/output/dense/MatMul:0:0 */;
  %672 = nn.dense(%670, %671, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.8/attention/output/dense/MatMul:0:0 */;
  %673 = reshape(%672, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/attention/output/dense/MatMul:0:0 */;
  %674 = add(%bert.encoder.layer.8.attention.output.dense.bias, %673) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/attention/output/dense/Add:0:0 */;
  %675 = add(%674, %632) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/attention/output/Add:0:0 */;
  %676 = mean(%675, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.8/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %677 = variance(%675, %676, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.8/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %678 = add(%677, 1e-12f /* ty=float32 span=/bert/encoder/layer.8/attention/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.8/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %679 = sqrt(%678) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.8/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %680 = subtract(%675, %676) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %681 = divide(1f /* ty=float32 span=/bert/encoder/layer.8/attention/output/LayerNorm/LayerNormalization:0:0 */, %679) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.8/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %682 = multiply(%680, %681) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %683 = multiply(%682, %bert.encoder.layer.8.attention.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %684 = add(%683, %bert.encoder.layer.8.attention.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %685 = reshape(%684, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.8/intermediate/dense/MatMul:0:0 */;
  %686 = transpose(%onnx::MatMul_1673, axes=None) /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.8/intermediate/dense/MatMul:0:0 */;
  %687 = nn.dense(%685, %686, units=None, out_dtype="float32") /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.8/intermediate/dense/MatMul:0:0 */;
  %688 = reshape(%687, newshape=[1, 512, 3072]) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.8/intermediate/dense/MatMul:0:0 */;
  %689 = add(%bert.encoder.layer.8.intermediate.dense.bias, %688) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.8/intermediate/dense/Add:0:0 */;
  %690 = divide(%689, 1.41421f /* ty=float32 span=/bert/encoder/layer.8/intermediate/intermediate_act_fn/Constant:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.8/intermediate/intermediate_act_fn/Div:0:0 */;
  %691 = erf(%690) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.8/intermediate/intermediate_act_fn/Erf:0:0 */;
  %692 = add(%691, 1f /* ty=float32 span=/bert/encoder/layer.8/intermediate/intermediate_act_fn/Constant_1:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.8/intermediate/intermediate_act_fn/Add:0:0 */;
  %693 = multiply(%689, %692) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.8/intermediate/intermediate_act_fn/Mul:0:0 */;
  %694 = multiply(%693, 0.5f /* ty=float32 span=/bert/encoder/layer.8/intermediate/intermediate_act_fn/Constant_2:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.8/intermediate/intermediate_act_fn/Mul_1:0:0 */;
  %695 = reshape(%694, newshape=[-1, 3072]) /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.8/output/dense/MatMul:0:0 */;
  %696 = transpose(%onnx::MatMul_1674, axes=None) /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.8/output/dense/MatMul:0:0 */;
  %697 = nn.dense(%695, %696, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.8/output/dense/MatMul:0:0 */;
  %698 = reshape(%697, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/output/dense/MatMul:0:0 */;
  %699 = add(%bert.encoder.layer.8.output.dense.bias, %698) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/output/dense/Add:0:0 */;
  %700 = add(%699, %684) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/output/Add:0:0 */;
  %701 = mean(%700, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.8/output/LayerNorm/LayerNormalization:0:0 */;
  %702 = variance(%700, %701, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.8/output/LayerNorm/LayerNormalization:0:0 */;
  %703 = add(%702, 1e-12f /* ty=float32 span=/bert/encoder/layer.8/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.8/output/LayerNorm/LayerNormalization:0:0 */;
  %704 = sqrt(%703) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.8/output/LayerNorm/LayerNormalization:0:0 */;
  %705 = subtract(%700, %701) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/output/LayerNorm/LayerNormalization:0:0 */;
  %706 = divide(1f /* ty=float32 span=/bert/encoder/layer.8/output/LayerNorm/LayerNormalization:0:0 */, %704) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.8/output/LayerNorm/LayerNormalization:0:0 */;
  %707 = multiply(%705, %706) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/output/LayerNorm/LayerNormalization:0:0 */;
  %708 = multiply(%707, %bert.encoder.layer.8.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/output/LayerNorm/LayerNormalization:0:0 */;
  %709 = add(%708, %bert.encoder.layer.8.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.8/output/LayerNorm/LayerNormalization:0:0 */;
  %710 = reshape(%709, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.9/attention/self/query/MatMul:0:0 */;
  %711 = transpose(%onnx::MatMul_1675, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.9/attention/self/query/MatMul:0:0 */;
  %712 = nn.dense(%710, %711, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.9/attention/self/query/MatMul:0:0 */;
  %713 = reshape(%712, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/attention/self/query/MatMul:0:0 */;
  %714 = add(%bert.encoder.layer.9.attention.self.query.bias, %713) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/attention/self/query/Add:0:0 */;
  %715 = reshape(%714, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.9/attention/self/Reshape:0:0 */;
  %716 = transpose(%715, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.9/attention/self/Transpose:0:0 */;
  %717 = multiply(%716, meta[relay.Constant][21] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.9/attention/self/Sqrt_1:0:0 */) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.9/attention/self/Mul:0:0 */;
  %718 = reshape(%709, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.9/attention/self/key/MatMul:0:0 */;
  %719 = transpose(%onnx::MatMul_1681, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.9/attention/self/key/MatMul:0:0 */;
  %720 = nn.dense(%718, %719, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.9/attention/self/key/MatMul:0:0 */;
  %721 = reshape(%720, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/attention/self/key/MatMul:0:0 */;
  %722 = add(%bert.encoder.layer.9.attention.self.key.bias, %721) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/attention/self/key/Add:0:0 */;
  %723 = reshape(%722, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.9/attention/self/Reshape_1:0:0 */;
  %724 = transpose(%723, axes=[0, 2, 3, 1]) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.9/attention/self/Transpose_2:0:0 */;
  %725 = multiply(%724, meta[relay.Constant][22] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.9/attention/self/Sqrt_2:0:0 */) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.9/attention/self/Mul_1:0:0 */;
  %726 = reshape(%725, newshape=[-1, 64, 512]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.9/attention/self/MatMul:0:0 */;
  %727 = reshape(%717, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.9/attention/self/MatMul:0:0 */;
  %728 = transpose(%726, axes=[0, 2, 1]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.9/attention/self/MatMul:0:0 */;
  %729 = nn.batch_matmul(%727, %728, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.9/attention/self/MatMul:0:0 */;
  %730 = reshape(%729, newshape=[1, 12, 512, 512]) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.9/attention/self/MatMul:0:0 */;
  %731 = add(%730, meta[relay.Constant][4] /* ty=Tensor[(1, 1, 512, 512), float32] span=/bert/Where_2:0:0 */) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.9/attention/self/Add:0:0 */;
  %732 = nn.softmax(%731, axis=3) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.9/attention/self/Softmax:0:0 */;
  %733 = reshape(%709, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.9/attention/self/value/MatMul:0:0 */;
  %734 = transpose(%onnx::MatMul_1687, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.9/attention/self/value/MatMul:0:0 */;
  %735 = nn.dense(%733, %734, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.9/attention/self/value/MatMul:0:0 */;
  %736 = reshape(%735, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/attention/self/value/MatMul:0:0 */;
  %737 = add(%bert.encoder.layer.9.attention.self.value.bias, %736) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/attention/self/value/Add:0:0 */;
  %738 = reshape(%737, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.9/attention/self/Reshape_2:0:0 */;
  %739 = transpose(%738, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.9/attention/self/Transpose_1:0:0 */;
  %740 = reshape(%739, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.9/attention/self/MatMul_1:0:0 */;
  %741 = reshape(%732, newshape=[-1, 512, 512]) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.9/attention/self/MatMul_1:0:0 */;
  %742 = transpose(%740, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.9/attention/self/MatMul_1:0:0 */;
  %743 = nn.batch_matmul(%741, %742, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.9/attention/self/MatMul_1:0:0 */;
  %744 = reshape(%743, newshape=[1, 12, 512, 64]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.9/attention/self/MatMul_1:0:0 */;
  %745 = transpose(%744, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.9/attention/self/Transpose_3:0:0 */;
  %746 = reshape(%745, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/attention/self/Reshape_3:0:0 */;
  %747 = reshape(%746, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.9/attention/output/dense/MatMul:0:0 */;
  %748 = transpose(%onnx::MatMul_1697, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.9/attention/output/dense/MatMul:0:0 */;
  %749 = nn.dense(%747, %748, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.9/attention/output/dense/MatMul:0:0 */;
  %750 = reshape(%749, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/attention/output/dense/MatMul:0:0 */;
  %751 = add(%bert.encoder.layer.9.attention.output.dense.bias, %750) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/attention/output/dense/Add:0:0 */;
  %752 = add(%751, %709) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/attention/output/Add:0:0 */;
  %753 = mean(%752, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.9/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %754 = variance(%752, %753, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.9/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %755 = add(%754, 1e-12f /* ty=float32 span=/bert/encoder/layer.9/attention/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.9/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %756 = sqrt(%755) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.9/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %757 = subtract(%752, %753) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %758 = divide(1f /* ty=float32 span=/bert/encoder/layer.9/attention/output/LayerNorm/LayerNormalization:0:0 */, %756) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.9/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %759 = multiply(%757, %758) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %760 = multiply(%759, %bert.encoder.layer.9.attention.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %761 = add(%760, %bert.encoder.layer.9.attention.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %762 = reshape(%761, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.9/intermediate/dense/MatMul:0:0 */;
  %763 = transpose(%onnx::MatMul_1698, axes=None) /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.9/intermediate/dense/MatMul:0:0 */;
  %764 = nn.dense(%762, %763, units=None, out_dtype="float32") /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.9/intermediate/dense/MatMul:0:0 */;
  %765 = reshape(%764, newshape=[1, 512, 3072]) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.9/intermediate/dense/MatMul:0:0 */;
  %766 = add(%bert.encoder.layer.9.intermediate.dense.bias, %765) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.9/intermediate/dense/Add:0:0 */;
  %767 = divide(%766, 1.41421f /* ty=float32 span=/bert/encoder/layer.9/intermediate/intermediate_act_fn/Constant:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.9/intermediate/intermediate_act_fn/Div:0:0 */;
  %768 = erf(%767) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.9/intermediate/intermediate_act_fn/Erf:0:0 */;
  %769 = add(%768, 1f /* ty=float32 span=/bert/encoder/layer.9/intermediate/intermediate_act_fn/Constant_1:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.9/intermediate/intermediate_act_fn/Add:0:0 */;
  %770 = multiply(%766, %769) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.9/intermediate/intermediate_act_fn/Mul:0:0 */;
  %771 = multiply(%770, 0.5f /* ty=float32 span=/bert/encoder/layer.9/intermediate/intermediate_act_fn/Constant_2:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.9/intermediate/intermediate_act_fn/Mul_1:0:0 */;
  %772 = reshape(%771, newshape=[-1, 3072]) /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.9/output/dense/MatMul:0:0 */;
  %773 = transpose(%onnx::MatMul_1699, axes=None) /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.9/output/dense/MatMul:0:0 */;
  %774 = nn.dense(%772, %773, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.9/output/dense/MatMul:0:0 */;
  %775 = reshape(%774, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/output/dense/MatMul:0:0 */;
  %776 = add(%bert.encoder.layer.9.output.dense.bias, %775) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/output/dense/Add:0:0 */;
  %777 = add(%776, %761) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/output/Add:0:0 */;
  %778 = mean(%777, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.9/output/LayerNorm/LayerNormalization:0:0 */;
  %779 = variance(%777, %778, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.9/output/LayerNorm/LayerNormalization:0:0 */;
  %780 = add(%779, 1e-12f /* ty=float32 span=/bert/encoder/layer.9/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.9/output/LayerNorm/LayerNormalization:0:0 */;
  %781 = sqrt(%780) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.9/output/LayerNorm/LayerNormalization:0:0 */;
  %782 = subtract(%777, %778) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/output/LayerNorm/LayerNormalization:0:0 */;
  %783 = divide(1f /* ty=float32 span=/bert/encoder/layer.9/output/LayerNorm/LayerNormalization:0:0 */, %781) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.9/output/LayerNorm/LayerNormalization:0:0 */;
  %784 = multiply(%782, %783) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/output/LayerNorm/LayerNormalization:0:0 */;
  %785 = multiply(%784, %bert.encoder.layer.9.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/output/LayerNorm/LayerNormalization:0:0 */;
  %786 = add(%785, %bert.encoder.layer.9.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.9/output/LayerNorm/LayerNormalization:0:0 */;
  %787 = reshape(%786, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.10/attention/self/query/MatMul:0:0 */;
  %788 = transpose(%onnx::MatMul_1700, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.10/attention/self/query/MatMul:0:0 */;
  %789 = nn.dense(%787, %788, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.10/attention/self/query/MatMul:0:0 */;
  %790 = reshape(%789, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/attention/self/query/MatMul:0:0 */;
  %791 = add(%bert.encoder.layer.10.attention.self.query.bias, %790) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/attention/self/query/Add:0:0 */;
  %792 = reshape(%791, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.10/attention/self/Reshape:0:0 */;
  %793 = transpose(%792, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.10/attention/self/Transpose:0:0 */;
  %794 = multiply(%793, meta[relay.Constant][23] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.10/attention/self/Sqrt_1:0:0 */) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.10/attention/self/Mul:0:0 */;
  %795 = reshape(%786, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.10/attention/self/key/MatMul:0:0 */;
  %796 = transpose(%onnx::MatMul_1706, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.10/attention/self/key/MatMul:0:0 */;
  %797 = nn.dense(%795, %796, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.10/attention/self/key/MatMul:0:0 */;
  %798 = reshape(%797, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/attention/self/key/MatMul:0:0 */;
  %799 = add(%bert.encoder.layer.10.attention.self.key.bias, %798) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/attention/self/key/Add:0:0 */;
  %800 = reshape(%799, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.10/attention/self/Reshape_1:0:0 */;
  %801 = transpose(%800, axes=[0, 2, 3, 1]) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.10/attention/self/Transpose_2:0:0 */;
  %802 = multiply(%801, meta[relay.Constant][24] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.10/attention/self/Sqrt_2:0:0 */) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.10/attention/self/Mul_1:0:0 */;
  %803 = reshape(%802, newshape=[-1, 64, 512]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.10/attention/self/MatMul:0:0 */;
  %804 = reshape(%794, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.10/attention/self/MatMul:0:0 */;
  %805 = transpose(%803, axes=[0, 2, 1]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.10/attention/self/MatMul:0:0 */;
  %806 = nn.batch_matmul(%804, %805, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.10/attention/self/MatMul:0:0 */;
  %807 = reshape(%806, newshape=[1, 12, 512, 512]) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.10/attention/self/MatMul:0:0 */;
  %808 = add(%807, meta[relay.Constant][4] /* ty=Tensor[(1, 1, 512, 512), float32] span=/bert/Where_2:0:0 */) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.10/attention/self/Add:0:0 */;
  %809 = nn.softmax(%808, axis=3) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.10/attention/self/Softmax:0:0 */;
  %810 = reshape(%786, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.10/attention/self/value/MatMul:0:0 */;
  %811 = transpose(%onnx::MatMul_1712, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.10/attention/self/value/MatMul:0:0 */;
  %812 = nn.dense(%810, %811, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.10/attention/self/value/MatMul:0:0 */;
  %813 = reshape(%812, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/attention/self/value/MatMul:0:0 */;
  %814 = add(%bert.encoder.layer.10.attention.self.value.bias, %813) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/attention/self/value/Add:0:0 */;
  %815 = reshape(%814, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.10/attention/self/Reshape_2:0:0 */;
  %816 = transpose(%815, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.10/attention/self/Transpose_1:0:0 */;
  %817 = reshape(%816, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.10/attention/self/MatMul_1:0:0 */;
  %818 = reshape(%809, newshape=[-1, 512, 512]) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.10/attention/self/MatMul_1:0:0 */;
  %819 = transpose(%817, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.10/attention/self/MatMul_1:0:0 */;
  %820 = nn.batch_matmul(%818, %819, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.10/attention/self/MatMul_1:0:0 */;
  %821 = reshape(%820, newshape=[1, 12, 512, 64]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.10/attention/self/MatMul_1:0:0 */;
  %822 = transpose(%821, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.10/attention/self/Transpose_3:0:0 */;
  %823 = reshape(%822, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/attention/self/Reshape_3:0:0 */;
  %824 = reshape(%823, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.10/attention/output/dense/MatMul:0:0 */;
  %825 = transpose(%onnx::MatMul_1722, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.10/attention/output/dense/MatMul:0:0 */;
  %826 = nn.dense(%824, %825, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.10/attention/output/dense/MatMul:0:0 */;
  %827 = reshape(%826, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/attention/output/dense/MatMul:0:0 */;
  %828 = add(%bert.encoder.layer.10.attention.output.dense.bias, %827) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/attention/output/dense/Add:0:0 */;
  %829 = add(%828, %786) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/attention/output/Add:0:0 */;
  %830 = mean(%829, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.10/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %831 = variance(%829, %830, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.10/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %832 = add(%831, 1e-12f /* ty=float32 span=/bert/encoder/layer.10/attention/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.10/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %833 = sqrt(%832) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.10/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %834 = subtract(%829, %830) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %835 = divide(1f /* ty=float32 span=/bert/encoder/layer.10/attention/output/LayerNorm/LayerNormalization:0:0 */, %833) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.10/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %836 = multiply(%834, %835) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %837 = multiply(%836, %bert.encoder.layer.10.attention.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %838 = add(%837, %bert.encoder.layer.10.attention.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %839 = reshape(%838, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.10/intermediate/dense/MatMul:0:0 */;
  %840 = transpose(%onnx::MatMul_1723, axes=None) /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.10/intermediate/dense/MatMul:0:0 */;
  %841 = nn.dense(%839, %840, units=None, out_dtype="float32") /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.10/intermediate/dense/MatMul:0:0 */;
  %842 = reshape(%841, newshape=[1, 512, 3072]) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.10/intermediate/dense/MatMul:0:0 */;
  %843 = add(%bert.encoder.layer.10.intermediate.dense.bias, %842) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.10/intermediate/dense/Add:0:0 */;
  %844 = divide(%843, 1.41421f /* ty=float32 span=/bert/encoder/layer.10/intermediate/intermediate_act_fn/Constant:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.10/intermediate/intermediate_act_fn/Div:0:0 */;
  %845 = erf(%844) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.10/intermediate/intermediate_act_fn/Erf:0:0 */;
  %846 = add(%845, 1f /* ty=float32 span=/bert/encoder/layer.10/intermediate/intermediate_act_fn/Constant_1:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.10/intermediate/intermediate_act_fn/Add:0:0 */;
  %847 = multiply(%843, %846) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.10/intermediate/intermediate_act_fn/Mul:0:0 */;
  %848 = multiply(%847, 0.5f /* ty=float32 span=/bert/encoder/layer.10/intermediate/intermediate_act_fn/Constant_2:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.10/intermediate/intermediate_act_fn/Mul_1:0:0 */;
  %849 = reshape(%848, newshape=[-1, 3072]) /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.10/output/dense/MatMul:0:0 */;
  %850 = transpose(%onnx::MatMul_1724, axes=None) /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.10/output/dense/MatMul:0:0 */;
  %851 = nn.dense(%849, %850, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.10/output/dense/MatMul:0:0 */;
  %852 = reshape(%851, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/output/dense/MatMul:0:0 */;
  %853 = add(%bert.encoder.layer.10.output.dense.bias, %852) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/output/dense/Add:0:0 */;
  %854 = add(%853, %838) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/output/Add:0:0 */;
  %855 = mean(%854, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.10/output/LayerNorm/LayerNormalization:0:0 */;
  %856 = variance(%854, %855, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.10/output/LayerNorm/LayerNormalization:0:0 */;
  %857 = add(%856, 1e-12f /* ty=float32 span=/bert/encoder/layer.10/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.10/output/LayerNorm/LayerNormalization:0:0 */;
  %858 = sqrt(%857) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.10/output/LayerNorm/LayerNormalization:0:0 */;
  %859 = subtract(%854, %855) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/output/LayerNorm/LayerNormalization:0:0 */;
  %860 = divide(1f /* ty=float32 span=/bert/encoder/layer.10/output/LayerNorm/LayerNormalization:0:0 */, %858) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.10/output/LayerNorm/LayerNormalization:0:0 */;
  %861 = multiply(%859, %860) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/output/LayerNorm/LayerNormalization:0:0 */;
  %862 = multiply(%861, %bert.encoder.layer.10.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/output/LayerNorm/LayerNormalization:0:0 */;
  %863 = add(%862, %bert.encoder.layer.10.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.10/output/LayerNorm/LayerNormalization:0:0 */;
  %864 = reshape(%863, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.11/attention/self/query/MatMul:0:0 */;
  %865 = transpose(%onnx::MatMul_1725, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.11/attention/self/query/MatMul:0:0 */;
  %866 = nn.dense(%864, %865, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.11/attention/self/query/MatMul:0:0 */;
  %867 = reshape(%866, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/attention/self/query/MatMul:0:0 */;
  %868 = add(%bert.encoder.layer.11.attention.self.query.bias, %867) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/attention/self/query/Add:0:0 */;
  %869 = reshape(%868, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.11/attention/self/Reshape:0:0 */;
  %870 = transpose(%869, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.11/attention/self/Transpose:0:0 */;
  %871 = multiply(%870, meta[relay.Constant][25] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.11/attention/self/Sqrt_1:0:0 */) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.11/attention/self/Mul:0:0 */;
  %872 = reshape(%863, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.11/attention/self/key/MatMul:0:0 */;
  %873 = transpose(%onnx::MatMul_1731, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.11/attention/self/key/MatMul:0:0 */;
  %874 = nn.dense(%872, %873, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.11/attention/self/key/MatMul:0:0 */;
  %875 = reshape(%874, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/attention/self/key/MatMul:0:0 */;
  %876 = add(%bert.encoder.layer.11.attention.self.key.bias, %875) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/attention/self/key/Add:0:0 */;
  %877 = reshape(%876, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.11/attention/self/Reshape_1:0:0 */;
  %878 = transpose(%877, axes=[0, 2, 3, 1]) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.11/attention/self/Transpose_2:0:0 */;
  %879 = multiply(%878, meta[relay.Constant][26] /* ty=Tensor[(1), float32] span=/bert/encoder/layer.11/attention/self/Sqrt_2:0:0 */) /* ty=Tensor[(1, 12, 64, 512), float32] span=/bert/encoder/layer.11/attention/self/Mul_1:0:0 */;
  %880 = reshape(%879, newshape=[-1, 64, 512]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.11/attention/self/MatMul:0:0 */;
  %881 = reshape(%871, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.11/attention/self/MatMul:0:0 */;
  %882 = transpose(%880, axes=[0, 2, 1]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.11/attention/self/MatMul:0:0 */;
  %883 = nn.batch_matmul(%881, %882, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.11/attention/self/MatMul:0:0 */;
  %884 = reshape(%883, newshape=[1, 12, 512, 512]) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.11/attention/self/MatMul:0:0 */;
  %885 = add(%884, meta[relay.Constant][4] /* ty=Tensor[(1, 1, 512, 512), float32] span=/bert/Where_2:0:0 */) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.11/attention/self/Add:0:0 */;
  %886 = nn.softmax(%885, axis=3) /* ty=Tensor[(1, 12, 512, 512), float32] span=/bert/encoder/layer.11/attention/self/Softmax:0:0 */;
  %887 = reshape(%863, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.11/attention/self/value/MatMul:0:0 */;
  %888 = transpose(%onnx::MatMul_1737, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.11/attention/self/value/MatMul:0:0 */;
  %889 = nn.dense(%887, %888, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.11/attention/self/value/MatMul:0:0 */;
  %890 = reshape(%889, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/attention/self/value/MatMul:0:0 */;
  %891 = add(%bert.encoder.layer.11.attention.self.value.bias, %890) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/attention/self/value/Add:0:0 */;
  %892 = reshape(%891, newshape=[1, 512, 12, 64]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.11/attention/self/Reshape_2:0:0 */;
  %893 = transpose(%892, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.11/attention/self/Transpose_1:0:0 */;
  %894 = reshape(%893, newshape=[-1, 512, 64]) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.11/attention/self/MatMul_1:0:0 */;
  %895 = reshape(%886, newshape=[-1, 512, 512]) /* ty=Tensor[(12, 512, 512), float32] span=/bert/encoder/layer.11/attention/self/MatMul_1:0:0 */;
  %896 = transpose(%894, axes=[0, 2, 1]) /* ty=Tensor[(12, 64, 512), float32] span=/bert/encoder/layer.11/attention/self/MatMul_1:0:0 */;
  %897 = nn.batch_matmul(%895, %896, out_dtype="float32", transpose_b=True) /* ty=Tensor[(12, 512, 64), float32] span=/bert/encoder/layer.11/attention/self/MatMul_1:0:0 */;
  %898 = reshape(%897, newshape=[1, 12, 512, 64]) /* ty=Tensor[(1, 12, 512, 64), float32] span=/bert/encoder/layer.11/attention/self/MatMul_1:0:0 */;
  %899 = transpose(%898, axes=[0, 2, 1, 3]) /* ty=Tensor[(1, 512, 12, 64), float32] span=/bert/encoder/layer.11/attention/self/Transpose_3:0:0 */;
  %900 = reshape(%899, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/attention/self/Reshape_3:0:0 */;
  %901 = reshape(%900, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.11/attention/output/dense/MatMul:0:0 */;
  %902 = transpose(%onnx::MatMul_1747, axes=None) /* ty=Tensor[(768, 768), float32] span=/bert/encoder/layer.11/attention/output/dense/MatMul:0:0 */;
  %903 = nn.dense(%901, %902, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.11/attention/output/dense/MatMul:0:0 */;
  %904 = reshape(%903, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/attention/output/dense/MatMul:0:0 */;
  %905 = add(%bert.encoder.layer.11.attention.output.dense.bias, %904) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/attention/output/dense/Add:0:0 */;
  %906 = add(%905, %863) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/attention/output/Add:0:0 */;
  %907 = mean(%906, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.11/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %908 = variance(%906, %907, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.11/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %909 = add(%908, 1e-12f /* ty=float32 span=/bert/encoder/layer.11/attention/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.11/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %910 = sqrt(%909) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.11/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %911 = subtract(%906, %907) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %912 = divide(1f /* ty=float32 span=/bert/encoder/layer.11/attention/output/LayerNorm/LayerNormalization:0:0 */, %910) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.11/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %913 = multiply(%911, %912) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %914 = multiply(%913, %bert.encoder.layer.11.attention.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %915 = add(%914, %bert.encoder.layer.11.attention.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/attention/output/LayerNorm/LayerNormalization:0:0 */;
  %916 = reshape(%915, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.11/intermediate/dense/MatMul:0:0 */;
  %917 = transpose(%onnx::MatMul_1748, axes=None) /* ty=Tensor[(3072, 768), float32] span=/bert/encoder/layer.11/intermediate/dense/MatMul:0:0 */;
  %918 = nn.dense(%916, %917, units=None, out_dtype="float32") /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.11/intermediate/dense/MatMul:0:0 */;
  %919 = reshape(%918, newshape=[1, 512, 3072]) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.11/intermediate/dense/MatMul:0:0 */;
  %920 = add(%bert.encoder.layer.11.intermediate.dense.bias, %919) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.11/intermediate/dense/Add:0:0 */;
  %921 = divide(%920, 1.41421f /* ty=float32 span=/bert/encoder/layer.11/intermediate/intermediate_act_fn/Constant:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.11/intermediate/intermediate_act_fn/Div:0:0 */;
  %922 = erf(%921) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.11/intermediate/intermediate_act_fn/Erf:0:0 */;
  %923 = add(%922, 1f /* ty=float32 span=/bert/encoder/layer.11/intermediate/intermediate_act_fn/Constant_1:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.11/intermediate/intermediate_act_fn/Add:0:0 */;
  %924 = multiply(%920, %923) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.11/intermediate/intermediate_act_fn/Mul:0:0 */;
  %925 = multiply(%924, 0.5f /* ty=float32 span=/bert/encoder/layer.11/intermediate/intermediate_act_fn/Constant_2:0:0 */) /* ty=Tensor[(1, 512, 3072), float32] span=/bert/encoder/layer.11/intermediate/intermediate_act_fn/Mul_1:0:0 */;
  %926 = reshape(%925, newshape=[-1, 3072]) /* ty=Tensor[(512, 3072), float32] span=/bert/encoder/layer.11/output/dense/MatMul:0:0 */;
  %927 = transpose(%onnx::MatMul_1749, axes=None) /* ty=Tensor[(768, 3072), float32] span=/bert/encoder/layer.11/output/dense/MatMul:0:0 */;
  %928 = nn.dense(%926, %927, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/bert/encoder/layer.11/output/dense/MatMul:0:0 */;
  %929 = reshape(%928, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/output/dense/MatMul:0:0 */;
  %930 = add(%bert.encoder.layer.11.output.dense.bias, %929) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/output/dense/Add:0:0 */;
  %931 = add(%930, %915) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/output/Add:0:0 */;
  %932 = mean(%931, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.11/output/LayerNorm/LayerNormalization:0:0 */;
  %933 = variance(%931, %932, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.11/output/LayerNorm/LayerNormalization:0:0 */;
  %934 = add(%933, 1e-12f /* ty=float32 span=/bert/encoder/layer.11/output/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.11/output/LayerNorm/LayerNormalization:0:0 */;
  %935 = sqrt(%934) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.11/output/LayerNorm/LayerNormalization:0:0 */;
  %936 = subtract(%931, %932) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/output/LayerNorm/LayerNormalization:0:0 */;
  %937 = divide(1f /* ty=float32 span=/bert/encoder/layer.11/output/LayerNorm/LayerNormalization:0:0 */, %935) /* ty=Tensor[(1, 512, 1), float32] span=/bert/encoder/layer.11/output/LayerNorm/LayerNormalization:0:0 */;
  %938 = multiply(%936, %937) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/output/LayerNorm/LayerNormalization:0:0 */;
  %939 = multiply(%938, %bert.encoder.layer.11.output.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/output/LayerNorm/LayerNormalization:0:0 */;
  %940 = add(%939, %bert.encoder.layer.11.output.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/bert/encoder/layer.11/output/LayerNorm/LayerNormalization:0:0 */;
  %941 = reshape(%940, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/cls/predictions/transform/dense/MatMul:0:0 */;
  %942 = transpose(%onnx::MatMul_1750, axes=None) /* ty=Tensor[(768, 768), float32] span=/cls/predictions/transform/dense/MatMul:0:0 */;
  %943 = nn.dense(%941, %942, units=None, out_dtype="float32") /* ty=Tensor[(512, 768), float32] span=/cls/predictions/transform/dense/MatMul:0:0 */;
  %944 = reshape(%943, newshape=[1, 512, 768]) /* ty=Tensor[(1, 512, 768), float32] span=/cls/predictions/transform/dense/MatMul:0:0 */;
  %945 = add(%cls.predictions.transform.dense.bias, %944) /* ty=Tensor[(1, 512, 768), float32] span=/cls/predictions/transform/dense/Add:0:0 */;
  %946 = divide(%945, 1.41421f /* ty=float32 span=/cls/predictions/transform/transform_act_fn/Constant:0:0 */) /* ty=Tensor[(1, 512, 768), float32] span=/cls/predictions/transform/transform_act_fn/Div:0:0 */;
  %947 = erf(%946) /* ty=Tensor[(1, 512, 768), float32] span=/cls/predictions/transform/transform_act_fn/Erf:0:0 */;
  %948 = add(%947, 1f /* ty=float32 span=/cls/predictions/transform/transform_act_fn/Constant_1:0:0 */) /* ty=Tensor[(1, 512, 768), float32] span=/cls/predictions/transform/transform_act_fn/Add:0:0 */;
  %949 = multiply(%945, %948) /* ty=Tensor[(1, 512, 768), float32] span=/cls/predictions/transform/transform_act_fn/Mul:0:0 */;
  %950 = multiply(%949, 0.5f /* ty=float32 span=/cls/predictions/transform/transform_act_fn/Constant_2:0:0 */) /* ty=Tensor[(1, 512, 768), float32] span=/cls/predictions/transform/transform_act_fn/Mul_1:0:0 */;
  %951 = mean(%950, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/cls/predictions/transform/LayerNorm/LayerNormalization:0:0 */;
  %952 = variance(%950, %951, axis=[2], keepdims=True) /* ty=Tensor[(1, 512, 1), float32] span=/cls/predictions/transform/LayerNorm/LayerNormalization:0:0 */;
  %953 = add(%952, 1e-12f /* ty=float32 span=/cls/predictions/transform/LayerNorm/LayerNormalization:0:0 */) /* ty=Tensor[(1, 512, 1), float32] span=/cls/predictions/transform/LayerNorm/LayerNormalization:0:0 */;
  %954 = sqrt(%953) /* ty=Tensor[(1, 512, 1), float32] span=/cls/predictions/transform/LayerNorm/LayerNormalization:0:0 */;
  %955 = subtract(%950, %951) /* ty=Tensor[(1, 512, 768), float32] span=/cls/predictions/transform/LayerNorm/LayerNormalization:0:0 */;
  %956 = divide(1f /* ty=float32 span=/cls/predictions/transform/LayerNorm/LayerNormalization:0:0 */, %954) /* ty=Tensor[(1, 512, 1), float32] span=/cls/predictions/transform/LayerNorm/LayerNormalization:0:0 */;
  %957 = multiply(%955, %956) /* ty=Tensor[(1, 512, 768), float32] span=/cls/predictions/transform/LayerNorm/LayerNormalization:0:0 */;
  %958 = multiply(%957, %cls.predictions.transform.LayerNorm.weight) /* ty=Tensor[(1, 512, 768), float32] span=/cls/predictions/transform/LayerNorm/LayerNormalization:0:0 */;
  %959 = add(%958, %cls.predictions.transform.LayerNorm.bias) /* ty=Tensor[(1, 512, 768), float32] span=/cls/predictions/transform/LayerNorm/LayerNormalization:0:0 */;
  %960 = reshape(%959, newshape=[-1, 768]) /* ty=Tensor[(512, 768), float32] span=/cls/predictions/decoder/MatMul:0:0 */;
  %961 = transpose(%onnx::MatMul_1751, axes=None) /* ty=Tensor[(30522, 768), float32] span=/cls/predictions/decoder/MatMul:0:0 */;
  %962 = nn.dense(%960, %961, units=None, out_dtype="float32") /* ty=Tensor[(512, 30522), float32] span=/cls/predictions/decoder/MatMul:0:0 */;
  %963 = reshape(%962, newshape=[1, 512, 30522]) /* ty=Tensor[(1, 512, 30522), float32] span=/cls/predictions/decoder/MatMul:0:0 */;
  add(%cls.predictions.bias, %963) /* ty=Tensor[(1, 512, 30522), float32] span=/cls/predictions/decoder/Add:0:0 */
}

